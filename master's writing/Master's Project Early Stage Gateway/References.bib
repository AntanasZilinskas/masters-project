@article{Smith2020,
  author = {Smith, John},
  year = {2020},
  title = {Improving Solar Flare Forecasts with Advanced ML},
  journal = {Space Weather Journal},
  volume = {15},
  number = {3},
  pages = {234-245}
}

@article{Jones2018,
  author = {Jones, Amy},
  year = {2018},
  title = {Statistical Models for Solar Flare Prediction},
  journal = {Journal of Solar Physics},
  volume = {12},
  pages = {100-110}
}

@inproceedings{Vaswani2017,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki et al.},
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2017}
}

@article{Miller2019,
  author = {Miller, Sarah},
  year = {2019},
  title = {Deep Learning Approaches to Solar Forecasting},
  journal = {International Journal of Forecasting},
  volume = {45},
  pages = {50-60}
}

@article{Clark2021,
  author = {Clark, David},
  year = {2021},
  title = {Hybrid CNN-LSTM Models for Solar Flare Prediction},
  journal = {Astronomy and Astrophysics},
  volume = {190},
  pages = {22-35}
}

@article{RefWorks:RefID:1-gettelman1997future,
	author={Andrew Gettelman and Alan J. Geer and Richard M. Forbes and Greg R. Carmichael and Graham Feingold and Derek J. Posselt and Graeme L. Stephens and Susan C. Van Den Heever and Adam C. Varble and Paquita Zuidema},
	year={1997},
	title={The future of Earth system prediction: Advances in model-data fusion},
	journal={Science Advances},
	volume={8},
	number={14},
	abstract={Predictions of the Earth system, such as weather forecasts and climate projections, require models informed by observations at many levels. Some methods for integrating models and observations are very systematic and comprehensive (e.g., data assimilation), and some are single purpose and customized (e.g., for model validation). We review current methods and best practices for integrating models and observations. We highlight how future developments can enable advanced heterogeneous observation networks and models to improve predictions of the Earth system (including atmosphere, land surface, oceans, cryosphere, and chemistry) across scales from weather to climate. As the community pushes to develop the next generation of models and data systems, there is a need to take a more holistic, integrated, and coordinated approach to models, observations, and their uncertainties to maximize the benefit for Earth system prediction and impacts on society. SIGN UP FOR THE AWARD-WINNING SCIENCEADVISER NEWSLETTER The latest news, commentary, and research, free to your inbox daily  REVIEW ATMOSPHERIC SCIENCE},
	doi={10.1126/sciadv.abn3488}
}







@article{RefWorks:RefID:35-licllmate:,
	author={Haobo Li and Zhaowei Wang and Jiachen Wang and Alexis Kai and Hon Lau and Huamin Qu},
	title={CLLMate: A Multimodal LLM for Weather and Climate Events Forecasting},
	abstract={Forecasting weather and climate events is crucial for making appropriate measures to mitigate environmental hazards and minimize associated losses. Previous research on environmental forecasting focuses on predicting numerical meteorological variables related to closed-set events rather than forecasting open-set events directly, which limits the comprehensiveness of event forecasting. We propose Weather and Climate Event Forecasting (WCEF), a new task that leverages meteorological raster data and textual event data to predict potential weather and climate events. However, due to di!culties in aligning multimodal data and the lack of su!cient supervised datasets, this task is challenging to accomplish. Therefore, we "rst propose a framework to align historical meteorological data with past weather and climate events using the large language model (LLM). In this framework, we construct a knowledge graph by using LLM to extract information about weather and climate events from a corpus of over 41k highly environment-focused news articles. Subsequently, we mapped these events with meteorological raster data, creating a supervised dataset, which is the largest and most novel for LLM tuning on the WCEF task. Finally, we introduced our aligned models, CLLMate (LLM for climate), a multimodal LLM to forecast weather and climate events using meteorological raster data. In evaluating CLLMate, we conducted extensive experiments. The results indicate that CLLMate surpasses both the baselines and other multimodal LLMs, showcasing the potential of utilizing LLM to align weather and climate events with meteorological data and highlighting the promising future for research on the WCEF task. As a representative hub in the "My Climate Risk" lighthouse activity initiated by the World Climate Research Programme (WCRP), we contribute CLLMate as a component of our regional solution to help forecast environmental risks and mitigate their loss.}
}
@misc{RefWorks:RefID:34-gallagheri.,
	author = 	 {Peter T. Gallagher and Y. -J Moon and Haimin Wang},
	title = 	 {I. Data Processing and First Results},
	abstract = 	 {This paper discusses a near real-time approach to solar active-region monitoring and flare prediction using the Big Bear Solar Observatory Active Region Monitor (ARM). Every hour, ARM reads, calibrates, and analyses a variety of data including: full-disk Hα images from the Global Hα Network; EUV, continuum, and magnetogram data from the Solar and Heliospheric Observatory (SOHO); and full-disk magnetograms from the Global Oscillation Network Group (GONG). For the first time, magnetic gradient maps derived from GONG longitudinal magnetograms are now available on-line and are found to be a useful diagnostic of flare activity. ARM also includes a variety of active-region properties from the National Oceanic and Atmospheric Administration's Space Environment Center, such as up-to-date active-region positions, GOES 5-min X-ray data, and flare-to-region identifications. Furthermore, we have developed a Flare Prediction System which estimates the probability for each region to produce CM M-, or X-class flares based on nearly eight years of NOAA data from cycle 22. This, in addition to BBSO's daily solar activity reports, has proven a useful resource for activity forecasting.}
}
@article{RefWorks:RefID:33-francisco2024multimodal,
	author={G. Francisco and S. Guastavino and J. Fernandes and T. Barata and D. Del Moro},
	year={2024},
	month={-10-22},
	title={Multimodal Flare Forecasting with Deep Learning},
	abstract={Solar flare forecasting mainly relies on photospheric magnetograms and associated physical features to predict forthcoming flares. However, it is believed that flare initiation mechanisms often originate in the chromosphere and the lower corona. In this study, we employ deep learning as a purely data-driven approach to compare the predictive capabilities of chromospheric and coronal UV and EUV emissions across different wavelengths with those of photospheric line-of-sight magnetograms. Our findings indicate that individual EUV wavelengths can provide discriminatory power comparable or better to that of line-of-sight magnetograms. Moreover, we identify simple multimodal neural network architectures that consistently outperform single-input models, showing complementarity between the flare precursors that can be extracted from the distinct layers of the solar atmosphere. To mitigate potential biases from known misattributions in Active Region flare catalogs, our models are trained and evaluated using full-disk images and a comprehensive flare event catalog at the full-disk level. We introduce a deep-learning architecture suited for extracting temporal features from full-disk videos.}
}
@article{RefWorks:RefID:32-zhangmm-llms:,
	author={Duzhen Zhang and Yahan Yu and Jiahua Dong and Chenxing Li and Dan Su and Chenhui Chu and Dong Yu},
	title={MM-LLMs: Recent Advances in MultiModal Large Language Models},
	abstract={In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website 1 for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.}
}
@article{RefWorks:RefID:31-mizrahi4m:,
	author={David Mizrahi and Roman Bachmann and Oguzhan Fatih Kar and Teresa Yeo and Mingfei Gao and Afshin Dehghan and Amir Zamir},
	title={4M: Massively Multimodal Masked Modeling},
	abstract={Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities-including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility. Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.}
}
@article{RefWorks:RefID:30-schmude2024prithvi,
	author={Johannes Schmude and Sujit Roy and Will Trojak and Johannes Jakubik and Daniel Salles Civitarese and Shraddha Singh and Julian Kuehnert and Kumar Ankur and Aman Gupta and Christopher E. Phillips and Romeo Kienzler and Daniela Szwarcman and Vishal Gaur and Rajat Shinde and Rohit Lal and Arlindo Da Silva and Jorge Luis and Guevara Diaz and Anne Jones and Simon Pfreundschuh and Amy Lin and Aditi Sheshadri and Udaysankar Nair and Valentine Anantharaj and Hendrik Hamann and Campbell Watson and Manil Maskey and Tsengdar J. Lee and Juan Bernabe Moreno and Rahul Ramachandran},
	year={2024},
	month={-09-20},
	title={Prithvi WxC: Foundation Model for Weather and Climate},
	abstract={Triggered by the realization that AI emulators can rival the performance of traditional numerical weather prediction models running on HPC systems, there is now an increasing number of large AI models that address use cases such as forecasting, downscaling, or nowcasting. While the parallel developments in the AI literature focus on foundation models-models that can be effectively tuned to address multiple, different use cases-the developments on the weather and climate side largely focus on single-use cases with particular emphasis on mid-range forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture, incorporating concepts from various recent transformer models to effectively capture both regional and global dependencies in the input data. The model has been designed to accommodate large token counts to model weather phenomena in different topologies at fine resolutions. Furthermore, it is trained with a mixed objective that combines the paradigms of masked reconstruction with forecasting. We test the model on a set of challenging downstream tasks namely: Autoregressive rollout forecasting, Downscaling, Gravity wave flux parameterization, and Extreme events estimation. The pretrained model with 2.3 billion parameters, along with the associated finetuning workflows, has been publicly released as an open-source contribution via Hugging Face.}
}
@article{RefWorks:RefID:29-hoffmanntraining,
	author={Jordan Hoffmann and Sebastian Borgeaud &#x1d43f; and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego De and Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George Van Den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
	title={Training Compute-Optimal Large Language Models},
	abstract={We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted computeoptimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4× more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.}
}
@misc{RefWorks:RefID:28-srivastava2023imitation,
	author = 	 {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal and Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka and Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart &#34; Omiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Yuchen Bill and Blake Lin and Bryan Howald and Cameron Orinion and Cameron Diao and Catherine Dour and Cedrick Stinson and César Argueta and Chandan Ferri Ramírez and Charles Singh and Chenlin Rathkopf and Chitta Meng and Chiyu Baral and Chris Wu and Chris Callison-Burch and Christian Waites and Christopher D. Voigt and Christopher Manning and Cindy Potts and Clara E. Ramirez and Clemencia Rivera and Colin Siro and #. Ra and Courtney El and Cristina Ashcraft and Damien Garbacea and Dan Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Daniel Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Danielle Moseguí González and Danny Perszyk and Danqi Hernandez and Daphne Chen and Dar Ippolito and David Gilboa and David Dohan and David Drakard and Debajyoti Jurgens and Deep Datta and Denis Ganguli and Denis Emelin and Deniz Kleyko and Derek Yuret and Derek Chen and Dieuwke Tam and Diganta Hupkes and Dilyar Misra and Dimitri Coelho Buzan and Diyi Mollo and Dong-Ho Yang and Dylan Lee and Ekaterina Schrader and Dogus Shutova and Elad Cubuk and Eleanor Segal and Elizabeth Hagerman and Elizabeth Barnes and Ellie Donoway and Emanuele Pavlick and Emma Rodola and Eric Lam and Eric Chu and Erkut Tang and Ernie Erdem and Ethan A. Chang and Ethan Chi and Ethan Dyer and Ethan Jerzak and Eunice Engefu Kim and Evgenii Manyasi and Fanyue Zheltonozhskii and Fatemeh Xia and Fernando Siar and Francesca Martínez-Plumed and Francois Happé and Frieda Chollet and Gaurav Rong and Mishra and Indra Genta and Gerard Winata and Germán De Melo and Giambattista Kruszewski and Giorgio Parascandolo and Gloria Mariani and Gonzalo Wang and Gregor Jaimovitch-López and Guy Betz and Hana Gur-Ari and Hannah Galijasevic and Hannah Kim and Hannaneh Rashkin and Harsh Hajishirzi and Hayden Mehta and Henry Bogar and Hinrich Shevlin and Hiromu Schütze and Hongming Yakura and Hugh Mee Zhang and Ian Wong and Isaac Ng and Jaap Noble and Jack Jumelet and Jackson Geissinger and Jacob Kernion and Jaehoon Hilton and Jaime Fernández Lee and James B. Fisac and James Simon and James Koppel and James Zheng and Jan Zou and Jana Koco$ and Janelle Thompson and Jared Wingfield and Jarema Kaplan and Jascha Radom and Jason Sohl-Dickstein and Jason Phang and Jason Wei and Jekaterina Yosinski and Jelle Novikova and Jennifer Bosscher and Jeremy Marsh and Jeroen Kim and Jesse Taal and Jesujoba Engel and Jiacheng Alabi and Jiaming Xu and Jillian Song and Joan Tang and John Waweru and John Burden and John U. Miller and Jonathan Balis and Jonathan Batchelder and Jörg Berant and Jos Frohberg and Jose Rozen and Joseph Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joshua B. Jones and Joshua S. Tenenbaum and Joyce Rule and Kamil Chua and Karen Kanclerz and Karl Livescu and Karthik Krauth and Katerina Gopalakrishnan and Katja Ignatyeva and Kaustubh D. Markert and Kevin Dhole and Kevin Gimpel and Kory Omondi and Kristen Mathewson and Ksenia Chiafullo and Kumar Shkaruta and Kyle Shridhar and Kyle Mcdonell and Laria Richardson and Leo Reynolds and Li Gao and Liam Zhang and Lianhui Dugan and Lidia Qin and Louis-Philippe Contreras-Ochando and Luca Morency and Lucas Moschella and Lucy Lam and Ludwig Noble and Luheng Schmidt and Luis Oliveros He and Luke Colón and Lütfi Metz and Kerem and Maarten Bosma and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Orduna Medina and Melody Baitemirova and Melvin Arnaud and Michael A. Mcelrath and Michael Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Micha &#34; Strube and Michele Sw&amp;drowski and Michihiro Bevilacqua and Mihir Yasunaga and Mike Kale and Mimee Cain and Mirac Xu and Mitch Suzgun and Mo Walker and Mohit Tiwari and Moin Bansal and Mor Aminnaseri and Mozhdeh Geva and Mukund Gheini and T. Varma and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur- and Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennigho# and Shirish Nitish and Niveditha S. Keskar and Noah Iyer and Noah Constant and Nuan Fiedel and Oliver Wen and Omar Zhang and Omar Agha and Omer Elbaghdadi and Owain Levy and Pablo Evans and Moreno Antonio and Parth Casares and Pascale Doshi and Paul Pu Fung and Paul Liang and Pegah Vicol and Peiyuan Alipoormolabashi and Percy Liao and Peter Liang and Peter Chang and Mon Eckersley and Pinyu Htut and Mi &#34; Hwang and Piyush Kowski and Pouya Patil and Priti Pezeshkpour and Qiaozhu Oli and Qing Mei and Qinlang Lyu and Rabin Chen and Rachel Etta Banjade and Raefer Rudolph and Rahel Gabriel and Ramon Habacker and Raphaël Risco and Rhythm Millière and Richard Garg and Rif A. Barnes and Riku Saurous and Robbe Arakawa and Robert Raymaekers and Rohan Frank and Roman Sikand and Roman Novak and Ronan Sitelew and Rosanne Lebras and Rowan Liu and Rui Jacobs and Ruslan Zhang and Ryan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Rylan Teehan and Sahib Yang and Saif M. Singh and Sajant Mohammad and Sam Anand and Sam Dillavou and Sam Shleifer and Samuel Wiseman and Samuel R. Gruetter and Samuel S. Bowman and Sanghyun Schoenholz and Sanjeev Han and Sarah A. Kwatra and Sarik Rous and Sayan Ghazarian and Sean Ghosh and Sebastian Casey and Bischo and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shane Shixiang and Shubh Gu and Shubham Pachchigar and Shyam Toshniwal and Shyamolima Upadhyay and Shammie Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
	year = 	 {2023},
	month = 	 {-06-12},
	title = 	 {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models Alphabetic author list: →}
}
@misc{RefWorks:RefID:27-minaeelarge,
	author = 	 {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
	title = 	 {Large Language Models: A Survey},
	abstract = 	 {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws [1], [2]. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.}
}
@article{RefWorks:RefID:26-liu2024datasets,
	author={Yang Liu and Jiahuan Cao and Chongyu Liu and Kai Ding and Lianwen Jin},
	year={2024},
	month={-02-28},
	title={Datasets for Large Language Models: A Comprehensive Survey},
	abstract={This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pretraining corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.}
}
@article{RefWorks:RefID:25-kaplanscaling,
	author={Jared Kaplan and Sam Mccandlish and ⇤ Openai and Tom Henighan and Tom B. Brown and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
	title={Scaling Laws for Neural Language Models},
	abstract={We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sampleefficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence. ⇤ Equal contribution. Contributions: Jared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM experiments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer implementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided guidance throughout the project.}
}
@misc{RefWorks:RefID:24-blumenfeldnasa,
	author = 	 {Josh Blumenfeld},
	title = 	 {NASA and IBM Research Apply AI to Weather and Climate | NASA Earthdata},
	abstract = 	 {The collaborative development of a weather and climate artificial intelligence (AI) foundation model supports a broad range of public safety and science applications.}
}

@article{RefWorks:RefID:22-gao2020pile:,
	author={Leo Gao and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
	year={2020},
	month={-12-31},
	title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
	abstract={Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present the Pile: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets-both existing and newly constructed-many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction. 1}
}
@misc{RefWorks:RefID:21-fedus2022switch,
	author = 	 {William Fedus and Barret Zoph and Noam Shazeer and Alexander Clark},
	year = 	 {2022},
	month = 	 {-06-16},
	title = 	 {Switch Transformers: Scaling to Trillion Parameter Models with Simple and E cient Sparsity},
	abstract = 	 {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select di↵erent parameters for each incoming example. The result is a sparsely-activated model-with an outrageous number of parameters-but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based o↵ T5-Base and T5-Large (Ra↵el et al., 2019) to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus", and achieve a 4x speedup over the T5-XXL model. 12}
}
@techreport{RefWorks:RefID:20-naveed2024comprehensive,
	author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
	year={2024},
	month={-10-18},
	title={A Comprehensive Overview of Large Language Models},
	abstract={Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive, informative summaries of the existing works to advance the LLM research.}
}
@article{RefWorks:RefID:19-guo2023evaluating,
	author={Zishan Guo and Jin → Renren and Chuang Liu and Yufei Huang and Dan Shi and Supryadi Linhao and Yan Liu and Jiaxuan Li and Bojian Xiong and Deyi Xiong},
	year={2023},
	month={-11-25},
	title={Evaluating Large Language Models: A Comprehensive Survey},
	abstract={Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could su!er from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To e!ectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.}
}
@article{RefWorks:RefID:18-rajbhandarizero:,
	author={Samyam Rajbhandari and Je↵ Rasley and Olatunji Ruwase and Yuxiong He},
	title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
	abstract={Large deep learning models o↵er significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development e ciency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be e ciently trained. ZeRO eliminates memory redundancies in data-and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high e ciency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (17B parameters) with record breaking accuracy.}
}
@article{RefWorks:RefID:17-devlinbert:,
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	abstract={We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{RefWorks:RefID:15-harra2016characteristics,
	author={Louise K. Harra and Carolus J. Schrijver and Miho Janvier and Shin Toriumi and Hugh Hudson and Sarah Matthews and Magnus M. Woods and Hirohisa Hara and Manuel Guedel and Adam Kowalski and Rachel Osten and Kanya Kusano and Theresa Lueftinger},
	year={2016},
	month={-06-27},
	title={The Characteristics of Solar X-Class Flares and CMEs: A Paradigm for Stellar Superflares and Eruptions?},
	journal={Solar Physics},
	volume={291},
	number={6},
	pages={1761},
	abstract={This paper explores the characteristics of 42 solar X-class flares that were observed between February 2011 and November 2014, with data from the Solar Dynamics Observatory (SDO) and other sources. This flare list includes nine X-class flares that had no associated CMEs. In particular our aim was to determine whether a clear signature could be identified to differentiate powerful flares that have coronal mass ejections (CMEs) from those that do not. Part of the motivation for this study is the characterization of the solar paradigm for flare/CME occurrence as a possible guide to the stellar observations; hence we emphasize spectroscopic signatures. To do this we ask the following questions: Do all eruptive flares have long durations? Do CME-related flares stand out in terms of active-region size vs. flare duration? Do flare magnitudes correlate with sunspot areas, and, if so, are eruptive events distinguished? Is the occurrence of CMEs related to the fraction of the activeregion area involved? Do X-class flares with no eruptions have weaker non-thermal signatures? Is the temperature dependence of evaporation different in eruptive and non-eruptive flares? Is EUV dimming only seen in eruptive flares? We find only one feature consistently associated with CME-related flares specifically: coronal dimming in lines characteristic of B L.K. Harra},
	isbn={0038-0938},
	doi={10.1007/s11207-016-0923-0}
}
@article{RefWorks:RefID:14-hayes2021solar,
	author={Laura A. Hayes and Oscar S. D. O’hara and Sophie A. Murray and Peter T. Gallagher},
	year={2021},
	month={-11-04},
	title={Solar Flare Effects on the Earth’s Lower Ionosphere},
	journal={Solar Physics},
	volume={296},
	number={11},
	abstract={Solar flares significantly impact the conditions of the Earth's ionosphere. In particular, the sudden increase in X-ray flux during a flare penetrates down to the lowest-lying D-region and dominates ionization at these altitudes (≈ 60-100 km). Measurements of very low frequency (VLF: 3-30 kHz) radio waves that reflect at D-region altitudes provide a unique remote-sensing probe to investigate the D-region response to solar-flare emissions. Here, using a combination of VLF amplitude measurements at 24 kHz together with X-ray observations from the Geostationary Operational Environment Satellite (GOES) X-ray sensor, we present a large-scale statistical study of 334 solar-flare events and their impacts on the D-region over the past solar cycle. Focusing on both GOES broadband X-ray channels, we investigate how the flare peak fluxes and position on the solar disk dictate an ionospheric response and extend this to investigate the characteristic time delay between incident X-ray flux and the D-region response. We show that the VLF amplitude linearly correlates with both the 1-8 Å and 0.5-4 Å channels, with correlation coefficients of 0.80 and 0.79, respectively. For the two X-class flares in our sample, however, there appears to be a turnover in the linear relationship, similar to previous works. Unlike higher altitude ionospheric regions for which the location of the flare on the solar disk affects the ionospheric response, we find that the D-region response to solar flares does not depend on the flare location. By comparing the time delays between the peak X-ray fluxes in both GOES channels and VLF amplitudes, we find that there is an important difference between the D-region response and the X-ray spectral band. We also demonstrate for several flare events that show a negative time delay, the peak VLF amplitude matches with the impulsive 25-50 keV hard X-ray fluxes measured by the Ramaty High Energy Solar Spectroscopic Imager (RHESSI). These results highlight the importance of performing full spectral analysis when studying the ionospheric responses to solar flares.},
	isbn={0038-0938},
	doi={10.1007/s11207-021-01898-y}
}
@article{RefWorks:RefID:13-yıldız2023effect,
	author={Leyla Bulut Yıldız and Erdinç Timoçin},
	year={2023},
	month={-09},
	title={The Effect of Solar Flares on HF Radio Communications over Turkey},
	journal={Geomagnetism and Aeronomy},
	volume={63},
	number={5},
	pages={642},
	abstract={This study investigates the effect of solar flares on absorption of high frequency (HF) radio signals over Turkey. For this purpose, the highest affected frequency (HAF) values by 1 dB absorption due to solar X-ray flux over Turkey were analyzed for different phases of solar flare, different local times (LT), different solar flare classes and different days. The HAF and ∆HAF values were calculated from an empirical model using X-ray flux data with 1-min resolution measured by the Geostationary Operational Environmental Satellite-15 (GOES-15) and solar zenith angle data. The increase in X-ray flux density during the ascending phase of the solar flare causes a sudden and large increase in HAF values. During this phase of flare, the HAF has a logarithmic relationship with X-ray flux values. The HAF reaches its maximum values at the solar flare peak. During the descending phase of solar flare, the HAF values gradually decrease as X-ray flux density decrease. The local time has a significant effect on HF absorption. The greatest increase in ∆HAF values occurs around noon. Comparisons between solar flare classes show that the ∆HAF values increases significantly as the solar flare density increases. For different days of year, the value of ∆HAF increases with decreasing solar zenith angles and the mean ∆HAF has a linear relationship with the values of mean solar zenith angle. The results of this study are important because it is the first attempt to examine the effect of solar flares on HF absorption over Turkey.},
	isbn={0016-7932},
	doi={10.1134/s0016793222600746}
}
@article{RefWorks:RefID:12-zheng2023comparative,
	author={Yanfang Zheng and Weishu Qin and Xuebao Li and Yi Ling and Xusheng Huang and Xuefeng Li and Pengchao Yan and Shuainan Yan and Hengrui Lou},
	year={2023},
	month={-07-04},
	title={Comparative analysis of machine learning models for solar flare prediction},
	journal={Astrophysics and Space Science},
	volume={368},
	number={7},
	abstract={In this paper, we develop five machine learning models, neural network (NN), long short-term memory (LSTM), LSTM based on attention mechanism (LSTM-A), bidirectional LSTM (BLSTM), and BLSTM based on attention mechanism (BLSTM-A), for predicting whether a ≥C class or ≥M class flare will occur in an active region in the next 24 hr. We use the data base provided by the Space-weather Helioseismic and Magnetic Imager Active Region Patches of Solar Dynamic Observatory, including 10 magnetic field features of active regions from 2010 May 1 to 2018 September 13. The samples are labeled flare information (i.e. No-flare/C/M/X) using solar flare events catalogue provided by the Geostationary Operational Environmental Satellite and Solar Geophysical Data solar event reports. In addition, we generated 10 cross-validation sets from these data using the cross-validation method. Then, after training, validating, and testing our models, we compare the results with the true skill statistics (TSS) as the assessment metric. The main results are as follows. (1) The TSS scores for ≥C class are 0.5472 ± 0.0809, 0.6425 ± 0.0685, 0.6904 ± 0.0575, 0.6681 ± 0.0573, and 0.6833 ± 0.0531 for NN, LSTM, LSTM-A, BLSTM and BLSTM-A, respectively. The TSS scores for ≥M class are 0.5723 ± 0.1139, 0.6579 ± 0.0758, 0.5943 ± 0.0712, 0.6493 ± 0.0826, and 0.5932 ± 0.0723, respectively. (2) For the first time, we add an attention mechanism to BLSTM for flare prediction, which improves the performance of the model for ≥C class. (3) Among the five models, the prediction model based on deep learning algorithms is generally superior to the model based on the traditional machine learning algorithm. The performance of the LSTM models is comparable to that of the BLSTM models. In general, LSTM-A for ≥C class performs better than other models. In addition, we also discuss the influence of 10 features on LSTM-A, and we find that removing the least significant feature will result in better performance than using all 10 features together, and the TSS score of the model will improve to 0.7059 ± 0.0440.},
	isbn={0004-640X},
	doi={10.1007/s10509-023-04209-y}
}
@misc{RefWorks:RefID:11-esa,
	title = 	 {ESA - What are solar flares?}
}
@misc{RefWorks:RefID:10-jiao2020solar,
	author = 	 {Zhenbang Jiao and Hu Sun and Xiantong Wang and Ward Manchester and Tamas Gombosi and Alfred Hero and Yang Chen},
	year = 	 {2020},
	month = 	 {-05-15},
	title = 	 {Solar Flare Intensity Prediction With Machine Learning Models},
	journal = 	 {Space Weather},
	volume = 	 {18},
	number = 	 {7},
	abstract = 	 {We develop a mixed long short-term memory (LSTM) regression model to predict the maximum solar "are intensity within a 24-hr time window 0-24, 6-30, 12-36, and 24-48 hr ahead of time using 6, 12, 24, and 48 hr of data (predictors) for each Helioseismic and},
	isbn = 	 {1542-7390},
	doi={10.1029/2020sw002440}
}
@article{RefWorks:RefID:9-yazev2023solar,
	author={S. A. Yazev},
	year={2023},
	month={-12},
	title={Solar Flares and Their Terrestrial Manifestations},
	journal={Astronomy Reports},
	volume={67},
	number={S1},
	pages={S78},
	abstract={The paper is an overview about the phenomenon of solar-terrestrial connections. The influence of various types of solar activity on processes on Earth is briefly described.},
	isbn={1063-7729},
	doi={10.1134/s1063772923130115}
}
@misc{RefWorks:RefID:8-gallagheri.,
	author = 	 {Peter T. Gallagher and Y. -J Moon and Haimin Wang},
	title = 	 {I. Data Processing and First Results},
	abstract = 	 {This paper discusses a near real-time approach to solar active-region monitoring and flare prediction using the Big Bear Solar Observatory Active Region Monitor (ARM). Every hour, ARM reads, calibrates, and analyses a variety of data including: full-disk Hα images from the Global Hα Network; EUV, continuum, and magnetogram data from the Solar and Heliospheric Observatory (SOHO); and full-disk magnetograms from the Global Oscillation Network Group (GONG). For the first time, magnetic gradient maps derived from GONG longitudinal magnetograms are now available on-line and are found to be a useful diagnostic of flare activity. ARM also includes a variety of active-region properties from the National Oceanic and Atmospheric Administration's Space Environment Center, such as up-to-date active-region positions, GOES 5-min X-ray data, and flare-to-region identifications. Furthermore, we have developed a Flare Prediction System which estimates the probability for each region to produce CM M-, or X-class flares based on nearly eight years of NOAA data from cycle 22. This, in addition to BBSO's daily solar activity reports, has proven a useful resource for activity forecasting.}
}
@article{RefWorks:RefID:7-schrijver2009driving,
	author={Carolus J. Schrijver},
	year={2009},
	month={-03},
	title={Driving major solar flares and eruptions: A review},
	journal={Advances in Space Research},
	volume={43},
	number={5},
	pages={739},
	abstract={This review focuses on the processes that energize and trigger M-and X-class solar flares and associated flux-rope destabilizations. Numerical modeling of specific solar regions is hampered by uncertain coronal-field reconstructions and by poorly understood magnetic reconnection; these limitations result in uncertain estimates of field topology, energy, and helicity. The primary advances in understanding field destabilizations therefore come from the combination of generic numerical experiments with interpretation of sets of observations. These suggest a critical role for the emergence of twisted flux ropes into pre-existing strong field for many, if not all, of the active regions that produce M-or X-class flares. The flux and internal twist of the emerging ropes appear to play as important a role in determining whether an eruption will develop predominantly as flare, confined eruption, or CME, as do the properties of the embedding field. Based on reviewed literature, I outline a scenario for major flares and eruptions that combines flux-rope emergence, mass draining, near-surface reconnection, and the interaction with the surrounding field. Whether deterministic forecasting is in principle possible remains to be seen: to date no reliable such forecasts can be made. Large-sample studies based on long-duration, comprehensive observations of active regions from their emergence through their flaring phase are needed to help us better understand these complex phenomena.},
	isbn={0273-1177},
	doi={10.1016/j.asr.2008.11.004}
}
@misc{RefWorks:RefID:6-omatolaimpacts,
	author = 	 {*. Omatola and K. M. Okeme},
	title = 	 {Impacts of solar storms on energy and communications technologies},
	abstract = 	 {The sun brings and gives light and warmth to both the living and non-living on the earth, but it has a temper too. Solar flares, eruptions and other sun storms can have catastrophic impacts to technological systems around or on the earth. They have been known to knock out satellites, power supplies, communications and navigation systems. Damage to these systems can result in secondary effects that can disrupt virtually every major infrastructure dependent on them, including transportation, security and emergency response systems, telecommunications and other wireless networks and electronic equipment which can lead to significant economic losses. This article dissects and analyzes the various threats created by solar storms to electronic communications and electrical systems. Finally, the necessary precautions to be taken to reduce the impacts of the solar threats and hence the associated economic losses are mentioned as the society now becomes more dependent on high tech innovations that are most vulnerable to solar storm activity.},
	isbn = 	 {0975-508X CODEN (USA)}
}
@misc{RefWorks:RefID:4-types,
	title={What are the different types, or classes, of flares?},
	url={https://solar-center.stanford.edu/sid/activities/flare.html}
}
@misc{RefWorks:RefID:3-vaswani2023provided,
	author = 	 {Ashish Vaswani and → Google Brain and Noam Shazeer and Niki Parmar and Google Research and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez},
	year = 	 {2023},
	month = 	 {-08-02},
	title = 	 {Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need},
	abstract = 	 {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. → Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † Work performed while at Google Brain. ‡ Work performed while at Google Research.}
}
@article{RefWorks:RefID:2-abduallah2023operational,
	author={Yasser Abduallah and Jason T. L. Wang and Haimin Wang and Yan Xu},
	year={2023},
	month={-08-22},
	title={Operational prediction of solar flares using a transformer-based framework},
	journal={Scientific Reports},
	volume={13},
	number={1},
	abstract={Solar flares are explosions on the Sun. They happen when energy stored in magnetic fields around solar active regions (ARs) is suddenly released. Solar flares and accompanied coronal mass ejections are sources of space weather, which negatively affects a variety of technologies at or near Earth, ranging from blocking high-frequency radio waves used for radio communication to degrading power grid operations. Monitoring and providing early and accurate prediction of solar flares is therefore crucial for preparedness and disaster risk management. In this article, we present a transformerbased framework, named SolarFlareNet, for predicting whether an AR would produce a γ-class flare within the next 24 to 72 h. We consider three γ classes, namely the ≥M5.0 class, the ≥ M class and the ≥ C class, and build three transformers separately, each corresponding to a γ class. Each transformer is used to make predictions of its corresponding γ-class flares. The crux of our approach is to model data samples in an AR as time series and to use transformers to capture the temporal dynamics of the data samples. Each data sample consists of magnetic parameters taken from Space-weather HMI Active Region Patches (SHARP) and related data products. We survey flare events that occurred from May 2010 to December 2022 using the Geostationary Operational Environmental Satellite X-ray flare catalogs provided by the National Centers for Environmental Information (NCEI), and build a database of flares with identified ARs in the NCEI flare catalogs. This flare database is used to construct labels of the data samples suitable for machine learning. We further extend the deterministic approach to a calibration-based probabilistic forecasting method. The SolarFlareNet system is fully operational and is capable of making near real-time predictions of solar flares on the Web. Solar ares are sudden explosions of energy that occur on the Sun's surface. ey o#en occur in solar active regions (ARs), caused by strong magnetic elds typically associated with sunspot areas. Solar ares are categorized into ve classes A, B, C, M, and X, with A-class ares having the lowest intensity and X-class ares having the highest intensity. Major ares are usually accompanied by coronal mass ejections and solar energetic particles 1-7. ese eruptive events can have signi cant and harmful e%ects on or near Earth, damaging technologies, power grids, space stations, and human life 8-11. erefore, providing accurate and early forecasts of solar ares is crucial for disaster risk management, risk mitigation, and preparedness. Although a lot of e%ort has been devoted to are prediction 12-15 , developing accurate, operational near-realtime are forecasting systems remains a challenge. In the past, researchers designed statistical models for the prediction of ares based on the physical properties of active regions 16-18. With the availability of large amounts of are-related data 14 , researchers started using machine learning methods for are forecasting 3,19,20. More recently, deep learning, which is a sub eld of machine learning, has emerged and showed promising results in predicting solar eruptions, including solar ares 21,22. For example, Nishizuka et&al. 23 developed deep neural networks to forecast M-and C-class ares that would occur within 24 h using data downloaded from the Solar Dynamics Observatory (SDO) 24 and the Geostationary Operational Environmental Satellite (GOES). Sun et&al. 22 employed three-dimensional (3D) convolutional neural networks (CNNs) to forecast M-class and C-class ares using Space-weather HMI Active Region Patches (SHARP) 25 magnetograms downloaded from the Joint Science Operations Center (JSOC) accessible at http:// jsoc. stanf ord. edu/. Li et&al. 26 also adopted a CNN model to forecast M-class and C-class ares using},
	doi={10.1038/s41598-023-40884-1}
}
