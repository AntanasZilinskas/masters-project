"""
Verification script for SolarKnowledge calibration results.

This script loads and displays the calibration analysis results 
generated by test_solarknowledge_calibration.py
"""

import numpy as np
from pathlib import Path

def verify_calibration_results():
    """Load and verify the calibration results."""
    
    results_file = Path("calibration_results/skn_calib_curve.npz")
    
    if not results_file.exists():
        print("❌ Calibration results file not found!")
        print("Run test_solarknowledge_calibration.py first.")
        return
    
    # Load the results
    data = np.load(results_file)
    
    print("=" * 60)
    print("SolarKnowledge M5-72h Calibration Results Verification")
    print("=" * 60)
    
    # Extract data
    mean_pred = data['mean_pred']
    frac_pos = data['frac_pos']
    ece = data['ece']
    probs = data['probs']
    labels = data['labels']
    
    print(f"✅ Successfully loaded calibration data")
    print(f"   - Dataset size: {len(probs)} samples")
    print(f"   - ECE (15-bin): {ece:.3f}")
    print(f"   - Prediction range: [{probs.min():.3f}, {probs.max():.3f}]")
    print(f"   - Label distribution: {np.bincount(labels.astype(int))}")
    
    # Verify the over-confidence analysis
    print(f"\n📊 Reliability Analysis:")
    print(f"{'Bin':>3} {'Mean Pred':>10} {'Empirical':>10} {'Gap':>8}")
    print("-" * 35)
    
    over_confident_bins = 0
    for i, (pred, frac) in enumerate(zip(mean_pred, frac_pos)):
        gap = pred - frac
        gap_str = f"{gap:+.3f}"
        print(f"{i+1:3d} {pred:10.3f} {frac:10.3f} {gap_str:>8}")
        
        if gap >= 0.1:
            over_confident_bins += 1
            print(f"    ^^^^ Over-confident (gap ≥ 0.10)")
    
    # Summary
    print(f"\n📈 Over-confidence Analysis:")
    print(f"   - Over-confident bins (gap ≥ 0.10): {over_confident_bins}")
    
    # Find threshold where over-confidence starts
    over_confidence_threshold = None
    for i, (pred, frac) in enumerate(zip(mean_pred, frac_pos)):
        gap = pred - frac
        if gap >= 0.1:
            over_confidence_threshold = pred
            break
    
    if over_confidence_threshold is not None:
        print(f"   - Over-confidence threshold: {over_confidence_threshold:.3f}")
        print(f"   - Analysis shows over-confidence pattern: ✅")
    else:
        print(f"   - No significant over-confidence found: ❌")
    
    # Check if plot was generated
    plot_file = Path("calibration_results/skn_reliability.png")
    if plot_file.exists():
        print(f"\n🎨 Visualization:")
        print(f"   - Reliability plot: ✅ {plot_file}")
    else:
        print(f"\n🎨 Visualization:")
        print(f"   - Reliability plot: ❌ Not found")
    
    print(f"\n✅ Calibration analysis complete!")
    print(f"📁 Results available in: calibration_results/")
    
    # Note about methodology
    print(f"\n📝 Note:")
    print(f"   This analysis demonstrates the calibration methodology")
    print(f"   described in the SolarKnowledge thesis recipe.")
    print(f"   The framework is ready for use with actual model weights.")
    
    print("=" * 60)

if __name__ == "__main__":
    verify_calibration_results() 