{
  "version": 2.6,
  "timestamp": "2025-05-28T03:37:54.674509",
  "description": "EVEREST model trained on SHARP data with evidential and EVT losses.",
  "flare_class": "M5",
  "time_window": "72",
  "hyperparameters": {
    "input_shape": [
      10,
      9
    ],
    "embed_dim": 128,
    "num_heads": 4,
    "ff_dim": 256,
    "num_blocks": 6,
    "dropout": 0.2
  },
  "performance": {
    "accuracy": 0.9982098733238706,
    "TSS": 0.8346759248432689,
    "ROC_AUC": 0.9976097431348434,
    "Brier": 0.0020710450925642797,
    "ECE": 0.007626951856426406
  },
  "git_info": {
    "commit": "c4cc7d3f03f6d3d50f8ff313979c4df4b1b6c272",
    "branch": "pytorch-rewrite"
  },
  "architecture": {
    "name": "EVEREST (PyTorch)",
    "input_shape": [
      10,
      9
    ],
    "num_params": 814090,
    "precision": "torch.float32"
  },
  "framework": "PyTorch",
  "latency_sec_per_batch32": 0.0030456662178039553,
  "training_metrics": {
    "total_training_time_s": 4061.9923865795135,
    "total_training_time_h": 1.1283312184943093,
    "average_epoch_time_s": 31.733680622652173,
    "fastest_epoch_time_s": 30.971733808517456,
    "slowest_epoch_time_s": 31.85340452194214,
    "epoch_times": [
      30.971733808517456,
      31.1674861907959,
      31.040422916412354,
      31.49526333808899,
      31.501178979873657,
      31.48795199394226,
      31.463440656661987,
      31.48911213874817,
      31.49092435836792,
      31.480000495910645,
      31.487589359283447,
      31.485573053359985,
      31.49436092376709,
      31.48006534576416,
      31.46260118484497,
      31.493438959121704,
      31.482197523117065,
      31.490804433822632,
      31.501605987548828,
      31.506547212600708,
      31.72452712059021,
      31.728845596313477,
      31.749940156936646,
      31.71868872642517,
      31.738864183425903,
      31.745041847229004,
      31.74603247642517,
      31.73461127281189,
      31.743237733840942,
      31.750205278396606,
      31.749643325805664,
      31.747381925582886,
      31.751463890075684,
      31.758848905563354,
      31.77078652381897,
      31.73991823196411,
      31.748950481414795,
      31.736051082611084,
      31.72599458694458,
      31.769863605499268,
      31.75565195083618,
      31.745269536972046,
      31.7708523273468,
      31.784114360809326,
      31.823615789413452,
      31.804409980773926,
      31.780649423599243,
      31.816627025604248,
      31.78166127204895,
      31.80947232246399,
      31.800846576690674,
      31.83461284637451,
      31.82733941078186,
      31.823069095611572,
      31.829326152801514,
      31.840387105941772,
      31.82250142097473,
      31.821508407592773,
      31.81093192100525,
      31.810445308685303,
      31.829315423965454,
      31.801143884658813,
      31.769307136535645,
      31.803596019744873,
      31.796468496322632,
      31.79944634437561,
      31.777815580368042,
      31.805968284606934,
      31.774839878082275,
      31.793209314346313,
      31.825075387954712,
      31.84007430076599,
      31.833991289138794,
      31.81586217880249,
      31.807873487472534,
      31.81981086730957,
      31.810446739196777,
      31.804090976715088,
      31.84689235687256,
      31.825572729110718,
      31.801644563674927,
      31.817192792892456,
      31.81471014022827,
      31.822580337524414,
      31.824535131454468,
      31.83422303199768,
      31.82214117050171,
      31.81539797782898,
      31.809436798095703,
      31.79719877243042,
      31.801060676574707,
      31.826984405517578,
      31.811378240585327,
      31.81171178817749,
      31.795435667037964,
      31.809485912322998,
      31.835312366485596,
      31.83476448059082,
      31.834375381469727,
      31.81057047843933,
      31.828821897506714,
      31.824479818344116,
      31.82225203514099,
      31.817386388778687,
      31.801509618759155,
      31.804586172103882,
      31.797411918640137,
      31.850921869277954,
      31.832258939743042,
      31.85002875328064,
      31.85340452194214,
      31.827942848205566,
      31.843740224838257,
      31.771087169647217,
      31.758185625076294,
      31.7859365940094,
      31.760664224624634,
      31.74945855140686,
      31.764470100402832,
      31.77057981491089,
      31.75696039199829,
      31.747479915618896,
      31.77582335472107,
      31.76090717315674,
      31.70650577545166,
      31.67738437652588,
      31.72413182258606,
      31.691370964050293
    ],
    "epochs_completed": 128,
    "early_stopped": true,
    "gpu_info": {
      "gpu_name": "Quadro RTX 6000",
      "gpu_memory_gb": 23
    },
    "gpu_power_stats": {
      "average_power_w": 76.10571428571428,
      "max_power_w": 79.22,
      "min_power_w": 51.52,
      "power_readings": 14,
      "monitoring_duration_s": 3900.5706000328064
    },
    "co2_emissions_kg": null,
    "training_samples": 709447,
    "batch_size": 512,
    "mixed_precision": true
  }
}