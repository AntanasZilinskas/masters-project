# EVEREST Ablation Study Framework

This directory contains a comprehensive ablation study framework for the EVEREST solar flare prediction model, implementing the systematic experimental protocol described in the research paper.

## ðŸ“‹ Overview

The ablation study framework performs systematic one-factor-at-a-time ablations on the M5-72h benchmark (the most challenging with 1:1,560 class imbalance) to measure the incremental utility of each architectural component.

### Experimental Protocol

- **Target**: M5-class flares, 72-hour prediction window
- **Reproducibility**: 5 independent seeds (0-4) per variant
- **Training**: 120 epochs maximum with early stopping after 10 epochs (no TSS improvement)
- **Hyperparameters**: Frozen optimal values from HPO study
- **Loss weights**: Re-normalized when components are removed
- **Statistical testing**: Paired bootstrap tests (10,000 resamples) with 95% confidence intervals
- **Significance**: p < 0.05 threshold

## ðŸ§ª Ablation Variants

### Component Ablations (7 variants)

1. **Full Model** - Complete EVEREST with all components (baseline)
2. **â€“ Evidential Head** - Remove NIG (evidential) branch
3. **â€“ EVT Head** - Remove GPD (extreme value theory) branch  
4. **Mean Pool** - Replace attention bottleneck with mean pooling
5. **Cross-Entropy** - Disable focal loss (Î³ = 0)
6. **No Precursor** - Remove early-warning auxiliary head
7. **FP32 Training** - Disable mixed precision (AMP)

### Sequence Length Ablations (5 variants)

- **5 timesteps** - Truncated sequence length
- **7 timesteps** - Reduced sequence length
- **10 timesteps** - Current baseline
- **15 timesteps** - Extended sequence length
- **20 timesteps** - Maximum sequence length

## ðŸ“Š Evaluation Metrics

- **TSS** (True Skill Statistic) - Primary metric
- **F1** - F1 score
- **ECE** - Expected Calibration Error (15-bin)
- **Brier** - Brier score
- **Accuracy** - Overall accuracy
- **Precision/Recall** - Classification metrics
- **ROC AUC** - Area under ROC curve
- **Latency** - Inference time (milliseconds)

## ðŸš€ Quick Start

### Local Execution

```bash
# Run complete ablation study
python models/ablation/run_ablation_study.py

# Run specific variants only
python models/ablation/run_ablation_study.py --variants full_model no_evidential no_evt

# Run with specific seeds
python models/ablation/run_ablation_study.py --seeds 0 1 2

# Skip sequence length study
python models/ablation/run_ablation_study.py --no-sequence-study

# Run analysis only (no training)
python models/ablation/run_ablation_study.py --analysis-only

# Single experiment for debugging
python -m ablation.trainer --variant no_evidential --seed 0
python -m ablation.trainer --variant full_model --seed 0 --sequence seq_15
```

### Cluster Execution

```bash
# Submit all jobs to cluster
cd models/ablation/cluster
./submit_jobs.sh

# Component ablations only
./submit_jobs.sh --component-only

# Sequence length ablations only  
./submit_jobs.sh --sequence-only

# Dry run (show commands without executing)
./submit_jobs.sh --dry-run

# If PBS dependency issues occur, use simple version:
./submit_jobs_simple.sh
# Then manually submit analysis after array job completes:
qsub submit_analysis.pbs
```

## ðŸ“ Directory Structure

```
models/ablation/
â”œâ”€â”€ config.py                    # Configuration and hyperparameters
â”œâ”€â”€ trainer.py                   # Training logic for ablation experiments
â”œâ”€â”€ analysis.py                  # Statistical analysis and visualization
â”œâ”€â”€ run_ablation_study.py        # Main orchestration script
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ cluster/                     # Cluster submission scripts
â”‚   â”œâ”€â”€ submit_ablation_array.pbs
â”‚   â”œâ”€â”€ submit_analysis.pbs
â”‚   â””â”€â”€ submit_jobs.sh
â”œâ”€â”€ results/                     # Experiment outputs
â”‚   â””â”€â”€ ablation_{variant}_seed{N}/
â”‚       â”œâ”€â”€ results.json
â”‚       â”œâ”€â”€ training_history.csv
â”‚       â””â”€â”€ final_metrics.csv
â”œâ”€â”€ plots/                       # Analysis visualizations
â”‚   â”œâ”€â”€ ablation_tss_impact.png
â”‚   â”œâ”€â”€ sequence_length_analysis.png
â”‚   â”œâ”€â”€ significance_heatmap.png
â”‚   â””â”€â”€ effect_sizes.png
â”œâ”€â”€ logs/                        # Training logs
â””â”€â”€ trained_models/              # Saved model weights
```

## âš™ï¸ Configuration

### Optimal Hyperparameters (Frozen)

From HPO study, used across all ablations:

```python
OPTIMAL_HYPERPARAMS = {
    "embed_dim": 128,
    "num_blocks": 4, 
    "dropout": 0.353,
    "focal_gamma": 2.803,
    "learning_rate": 5.34e-4,
    "batch_size": 512
}
```

### Dynamic Weight Schedule

The framework uses a 3-phase dynamic weight schedule (matching main training exactly):

- **Phase 1 (epochs 0-19)**: `{"focal": 0.9, "evid": 0.1, "evt": 0.0, "prec": 0.05}` (sum = 1.05)
- **Phase 2 (epochs 20-39)**: `{"focal": 0.8, "evid": 0.1, "evt": 0.1, "prec": 0.05}` (sum = 1.05)
- **Phase 3 (epochs 40+)**: `{"focal": 0.7, "evid": 0.1, "evt": 0.2, "prec": 0.05}` (sum = 1.05)

When components are removed for ablations, the corresponding weights are simply set to 0.0 (no re-normalization).

## ðŸ“ˆ Statistical Analysis

### Bootstrap Testing

- **Method**: Paired bootstrap tests against full model baseline
- **Resamples**: 10,000 per comparison
- **Confidence**: 95% intervals
- **Significance**: p < 0.05 threshold

### Effect Size Calculation

Cohen's d effect sizes computed for all comparisons:
- Small effect: |d| â‰¥ 0.2
- Medium effect: |d| â‰¥ 0.5  
- Large effect: |d| â‰¥ 0.8

## ðŸ“Š Expected Results

Based on paper specifications, expected TSS deltas from full model:

| Variant | Expected Î” TSS | Significance |
|---------|----------------|--------------|
| â€“ Evidential | -0.094 | âœ“ |
| â€“ EVT | -0.067 | âœ“ |
| Mean Pool | -0.045 | âœ“ |
| Cross-Entropy | -0.023 | âœ“ |
| No Precursor | -0.012 | âœ“ |
| FP32 Training | -0.003 | âœ— |

## ðŸ–¥ï¸ Computational Requirements

### Per Experiment
- **GPU**: 1x V100 (32GB VRAM)
- **CPU**: 8 cores
- **Memory**: 32GB RAM
- **Time**: 2-4 hours
- **Storage**: ~1GB per experiment

### Full Study
- **Total experiments**: 60 (35 component + 25 sequence)
- **Parallel execution**: ~24 hours with 50 concurrent jobs
- **Total storage**: ~60GB
- **Analysis time**: Additional 2 hours

## ðŸ”§ Troubleshooting

### Common Issues

1. **PyTorch not found**: Ensure EVEREST environment is activated
   ```bash
   conda activate everest_env
   ```

2. **CUDA out of memory**: Reduce batch size in config
   ```python
   OPTIMAL_HYPERPARAMS["batch_size"] = 256
   ```

3. **Missing data**: Verify training/testing data files exist
   ```bash
   ls Nature_data/training_data_M5_72.csv
   ls Nature_data/testing_data_M5_72.csv
   ```

4. **PBS dependency error** (`qsub: illegal -W value`):
   ```bash
   # Use simple submission script instead:
   cd models/ablation/cluster
   ./submit_jobs_simple.sh
   
   # Then manually submit analysis after array completes:
   qsub submit_analysis.pbs
   ```

5. **Cluster job failures**: Check PBS logs
   ```bash
   tail -f logs/ablation_*.log
   ```

### Debug Mode

Run single experiment with detailed logging:
```bash
python -m ablation.trainer --variant full_model --seed 0 --verbose
```

## ðŸ“š Implementation Details

### Reproducibility

- Fixed random seeds control weight initialization, data shuffling, dropout masks
- Deterministic CUDA operations enabled
- Environment variables set for additional reproducibility

### Sequence Length Handling

- **Truncation**: For shorter sequences, take most recent timesteps
- **Padding**: For longer sequences, repeat first timestep

### Early Stopping

- Monitor TSS on test set (following paper protocol)
- Stop after 10 epochs without improvement
- Restore best weights from optimal epoch

## ðŸ¤ Contributing

To add new ablation variants:

1. Add configuration to `config.py`:
   ```python
   "new_variant": {
       "name": "New Variant",
       "description": "Description of ablation",
       "config": {
           "use_evidential": True,
           # ... other flags
           "loss_weights": {"focal": 0.65, "evid": 0.1, "evt": 0.2, "prec": 0.05}
       }
   }
   ```

2. Update cluster scripts if needed
3. Run validation: `python test_ablation_setup.py`

## ðŸ“„ Citation

If you use this ablation framework, please cite:

```bibtex
@article{everest2024,
  title={EVEREST: Enhanced Variational Extreme Rare-Event Simulation for Solar Flare Prediction},
  author={[Authors]},
  journal={[Journal]},
  year={2024}
}
```

## ðŸ“ž Support

For issues or questions:
- Check logs in `models/ablation/logs/`
- Review configuration in `models/ablation/config.py`
- Run test suite: `python test_ablation_setup.py`
- Open GitHub issue with error details 