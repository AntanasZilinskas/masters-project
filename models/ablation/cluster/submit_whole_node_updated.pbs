#!/bin/bash
#PBS -N everest_ablation_node
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=32:mem=128gb:ngpus=8
#PBS -q gpu72_8
#PBS -j oe
#PBS -o logs/ablation_node_${PBS_JOBID}.log

# EVEREST Ablation Study - Whole Node Approach (Updated for Imperial RCS)
# This script requests a full node with 8 GPUs and runs experiments in parallel
# Updated to use correct queue (gpu72_8) and maximum GPUs per node (8)

# Load environment
module load anaconda3/personal
source activate everest_env

# Set up paths
cd $PBS_O_WORKDIR
export PYTHONPATH="${PBS_O_WORKDIR}:${PYTHONPATH}"

# Create logs directory
mkdir -p logs
mkdir -p models/ablation/results

echo "=== EVEREST Ablation Study - Whole Node (Updated) ==="
echo "Job ID: $PBS_JOBID"
echo "Node: $(hostname)"
echo "Working Directory: $(pwd)"
echo "Available GPUs: $CUDA_VISIBLE_DEVICES"
echo "CPUs: 32"
echo "Memory: 128GB"
echo "Queue: gpu72_8 (8 GPUs max)"
echo "Start Time: $(date)"
echo ""

# Check GPU availability and type
nvidia-smi
echo ""

# Define experiment configurations
VARIANTS=("full_model" "no_evidential" "no_evt" "mean_pool" "cross_entropy" "no_precursor" "fp32_training")
SEEDS=(0 1 2 3 4)  # All 5 seeds for full statistical power
SEQ_VARIANTS=("seq_5" "seq_7" "seq_10" "seq_15" "seq_20")

# Create experiment queue
EXPERIMENTS=()

# Add component ablations
for variant in "${VARIANTS[@]}"; do
    for seed in "${SEEDS[@]}"; do
        EXPERIMENTS+=("component:$variant:$seed")
    done
done

# Add sequence length ablations
for seq in "${SEQ_VARIANTS[@]}"; do
    for seed in "${SEEDS[@]}"; do
        EXPERIMENTS+=("sequence:$seq:$seed")
    done
done

TOTAL_EXPERIMENTS=${#EXPERIMENTS[@]}
NUM_GPUS=$(echo $CUDA_VISIBLE_DEVICES | tr ',' '\n' | wc -l)

echo "ðŸ“Š Experiment Plan:"
echo "   Component ablations: $((${#VARIANTS[@]} * ${#SEEDS[@]}))"
echo "   Sequence ablations: $((${#SEQ_VARIANTS[@]} * ${#SEEDS[@]}))"
echo "   Total experiments: $TOTAL_EXPERIMENTS"
echo "   Available GPUs: $NUM_GPUS"
echo "   Updated hyperparameters: embed_dim=64, num_blocks=8, batch_size=1024"
echo "   GPU Types: L40S (48GB) or A100 (40GB)"
echo ""

# Function to run experiment on specific GPU
run_experiment_on_gpu() {
    local gpu_id=$1
    local experiment=$2
    local exp_num=$3
    
    # Parse experiment string
    IFS=':' read -r exp_type variant_or_seq seed <<< "$experiment"
    
    echo "ðŸ”¬ [GPU $gpu_id] [$exp_num/$TOTAL_EXPERIMENTS] Starting:"
    echo "   Type: $exp_type"
    echo "   Variant/Seq: $variant_or_seq"
    echo "   Seed: $seed"
    echo "   Time: $(date)"
    
    # Set GPU for this experiment
    export CUDA_VISIBLE_DEVICES=$gpu_id
    
    # Run the appropriate experiment
    if [ "$exp_type" = "component" ]; then
        timeout 7200 python -m ablation.trainer --variant $variant_or_seq --seed $seed
    else
        timeout 7200 python -m ablation.trainer --variant full_model --seed $seed --sequence $variant_or_seq
    fi
    
    local exit_code=$?
    
    if [ $exit_code -eq 0 ]; then
        echo "   âœ… [GPU $gpu_id] Completed successfully"
    elif [ $exit_code -eq 124 ]; then
        echo "   â° [GPU $gpu_id] Timeout after 2 hours"
    else
        echo "   âŒ [GPU $gpu_id] Failed with exit code $exit_code"
    fi
    
    return $exit_code
}

# Parallel execution manager
start_time=$(date +%s)
completed=0
failed=0
gpu_pids=()

# Get actual GPU IDs from CUDA_VISIBLE_DEVICES
IFS=',' read -ra GPU_ARRAY <<< "$CUDA_VISIBLE_DEVICES"
ACTUAL_NUM_GPUS=${#GPU_ARRAY[@]}

echo "ðŸŽ¯ Detected $ACTUAL_NUM_GPUS GPUs: ${GPU_ARRAY[*]}"

# Initialize GPU queues
declare -a gpu_queues
for i in $(seq 0 $((ACTUAL_NUM_GPUS-1))); do
    gpu_queues[$i]=""
done

# Distribute experiments across available GPUs
for i in "${!EXPERIMENTS[@]}"; do
    gpu_idx=$((i % ACTUAL_NUM_GPUS))
    gpu_queues[$gpu_idx]+="$i "
done

echo "ðŸš€ Starting parallel execution on $ACTUAL_NUM_GPUS GPUs..."
echo ""

# Launch experiments on each GPU
for gpu_idx in $(seq 0 $((ACTUAL_NUM_GPUS-1))); do
    gpu_id=${GPU_ARRAY[$gpu_idx]}
    {
        for exp_idx in ${gpu_queues[$gpu_idx]}; do
            run_experiment_on_gpu $gpu_id "${EXPERIMENTS[$exp_idx]}" $((exp_idx + 1))
            
            if [ $? -eq 0 ]; then
                ((completed++))
            else
                ((failed++))
            fi
            
            # Brief pause between experiments on same GPU
            sleep 30
        done
    } &
    
    gpu_pids[$gpu_idx]=$!
    echo "ðŸŽ¯ GPU $gpu_id: Started with PID ${gpu_pids[$gpu_idx]}"
done

echo ""
echo "â³ Waiting for all GPUs to complete..."

# Wait for all GPU processes to complete
for gpu_idx in $(seq 0 $((ACTUAL_NUM_GPUS-1))); do
    wait ${gpu_pids[$gpu_idx]}
    echo "âœ… GPU ${GPU_ARRAY[$gpu_idx]} completed"
done

# Summary
total_time=$(($(date +%s) - start_time))
echo ""
echo "ðŸŽ‰ Whole node execution completed!"
echo "   Total experiments: $TOTAL_EXPERIMENTS"
echo "   Completed successfully: $completed"
echo "   Failed: $failed"
echo "   Total time: $((total_time/3600))h $((total_time%3600/60))m"
echo "   Average time per experiment: $((total_time/TOTAL_EXPERIMENTS/60)) minutes"
echo "   GPU utilization: $ACTUAL_NUM_GPUS GPUs in parallel"
echo ""

# Check results
echo "ðŸ“Š Results summary:"
ls -la models/ablation/results/ | wc -l
echo "   Result directories created: $(ls -d models/ablation/results/ablation_* 2>/dev/null | wc -l)"

echo ""
echo "ðŸ“‹ Next steps:"
echo "   1. Check results: ls models/ablation/results/"
echo "   2. Run analysis: python run_updated_ablation.py --analysis-only"
echo "   3. Generate thesis figures and tables"
echo ""
echo "End Time: $(date)" 