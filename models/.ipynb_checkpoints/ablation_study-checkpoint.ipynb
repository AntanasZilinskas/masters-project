{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d981ce1-6061-49f5-8357-be4530d67bf0",
   "metadata": {},
   "source": [
    "Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd1e07f-75c3-425b-ad0c-ba972b647f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:20:02.772745: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-16 03:20:02.772823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-16 03:20:02.773926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 03:20:02.780988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 03:20:14.570866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-16 03:20:27.971981: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TensorFlow GPU device not found.\n",
      "TensorFlow backend version: 2.15.0\n",
      "SUCCESS: PyTorch found GPU: Quadro RTX 6000\n",
      "PyTorch CUDA version: 12.6\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Python version: 3.11.12\n",
      "\n",
      "\n",
      "==================================================\n",
      "Running full_model\n",
      "==================================================\n",
      "\n",
      "==============================\n",
      "â–¶ Running variant: full_model\n",
      "==============================\n",
      "Epoch 1/300 - loss: 0.0001 - acc: 0.9930 - tss: 0.0093 - gamma: 0.00\n",
      "Epoch 2/300 - loss: 0.0000 - acc: 0.9938 - tss: 0.1159 - gamma: 0.04\n",
      "Epoch 3/300 - loss: 0.0000 - acc: 0.9942 - tss: 0.1728 - gamma: 0.08\n",
      "Epoch 4/300 - loss: 0.0000 - acc: 0.9944 - tss: 0.2132 - gamma: 0.12\n",
      "Epoch 5/300 - loss: 0.0000 - acc: 0.9946 - tss: 0.2495 - gamma: 0.16\n",
      "Epoch 6/300 - loss: 0.0000 - acc: 0.9948 - tss: 0.2953 - gamma: 0.20\n",
      "Epoch 7/300 - loss: 0.0000 - acc: 0.9951 - tss: 0.3508 - gamma: 0.24\n",
      "Epoch 8/300 - loss: 0.0000 - acc: 0.9954 - tss: 0.3879 - gamma: 0.28\n",
      "Epoch 9/300 - loss: 0.0000 - acc: 0.9956 - tss: 0.4224 - gamma: 0.32\n",
      "Epoch 10/300 - loss: 0.0000 - acc: 0.9957 - tss: 0.4449 - gamma: 0.36\n",
      "Epoch 11/300 - loss: 0.0000 - acc: 0.9957 - tss: 0.4541 - gamma: 0.40\n",
      "Epoch 12/300 - loss: 0.0000 - acc: 0.9959 - tss: 0.4871 - gamma: 0.44\n",
      "Epoch 13/300 - loss: 0.0000 - acc: 0.9960 - tss: 0.4972 - gamma: 0.48\n",
      "Epoch 14/300 - loss: 0.0000 - acc: 0.9961 - tss: 0.5124 - gamma: 0.52\n",
      "Epoch 15/300 - loss: 0.0000 - acc: 0.9963 - tss: 0.5345 - gamma: 0.56\n",
      "Epoch 16/300 - loss: 0.0000 - acc: 0.9963 - tss: 0.5498 - gamma: 0.60\n",
      "Epoch 17/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5566 - gamma: 0.64\n",
      "Epoch 18/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5596 - gamma: 0.68\n",
      "Epoch 19/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5671 - gamma: 0.72\n",
      "Epoch 20/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5821 - gamma: 0.76\n",
      "Epoch 21/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5913 - gamma: 0.80\n",
      "Epoch 22/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5897 - gamma: 0.84\n",
      "Epoch 23/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5792 - gamma: 0.88\n",
      "Epoch 24/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.5945 - gamma: 0.92\n",
      "Epoch 25/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.5954 - gamma: 0.96\n",
      "Epoch 26/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6094 - gamma: 1.00\n",
      "Epoch 27/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6072 - gamma: 1.04\n",
      "Epoch 28/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6025 - gamma: 1.08\n",
      "Epoch 29/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6249 - gamma: 1.12\n",
      "Epoch 30/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6298 - gamma: 1.16\n",
      "Epoch 31/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6332 - gamma: 1.20\n",
      "Epoch 32/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6433 - gamma: 1.24\n",
      "Epoch 33/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6393 - gamma: 1.28\n",
      "Epoch 34/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6376 - gamma: 1.32\n",
      "Epoch 35/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6443 - gamma: 1.36\n",
      "Epoch 36/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6531 - gamma: 1.40\n",
      "Epoch 37/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6505 - gamma: 1.44\n",
      "Epoch 38/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6429 - gamma: 1.48\n",
      "Epoch 39/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6651 - gamma: 1.52\n",
      "Epoch 40/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6707 - gamma: 1.56\n",
      "Epoch 41/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6694 - gamma: 1.60\n",
      "Epoch 42/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6808 - gamma: 1.64\n",
      "Epoch 43/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6663 - gamma: 1.68\n",
      "Epoch 44/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6741 - gamma: 1.72\n",
      "Epoch 45/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7020 - gamma: 1.76\n",
      "Epoch 46/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6894 - gamma: 1.80\n",
      "Epoch 47/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6942 - gamma: 1.84\n",
      "Epoch 48/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6753 - gamma: 1.88\n",
      "Epoch 49/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7129 - gamma: 1.92\n",
      "Epoch 50/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7280 - gamma: 1.96\n",
      "Epoch 51/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7102 - gamma: 2.00\n",
      "Epoch 52/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7238 - gamma: 2.00\n",
      "Epoch 53/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7258 - gamma: 2.00\n",
      "Epoch 54/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7374 - gamma: 2.00\n",
      "Epoch 55/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7353 - gamma: 2.00\n",
      "Epoch 56/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7331 - gamma: 2.00\n",
      "Epoch 57/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7451 - gamma: 2.00\n",
      "Epoch 58/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7402 - gamma: 2.00\n",
      "Epoch 59/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7321 - gamma: 2.00\n",
      "Epoch 60/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7497 - gamma: 2.00\n",
      "Epoch 61/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7409 - gamma: 2.00\n",
      "Epoch 62/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7487 - gamma: 2.00\n",
      "Epoch 63/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7474 - gamma: 2.00\n",
      "Epoch 64/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7682 - gamma: 2.00\n",
      "Epoch 65/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7388 - gamma: 2.00\n",
      "Epoch 66/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7687 - gamma: 2.00\n",
      "Epoch 67/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7739 - gamma: 2.00\n",
      "Epoch 68/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7817 - gamma: 2.00\n",
      "Epoch 69/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7787 - gamma: 2.00\n",
      "Epoch 70/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7859 - gamma: 2.00\n",
      "Epoch 71/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7628 - gamma: 2.00\n",
      "Epoch 72/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7755 - gamma: 2.00\n",
      "Epoch 73/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7868 - gamma: 2.00\n",
      "Epoch 74/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7823 - gamma: 2.00\n",
      "Epoch 75/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7887 - gamma: 2.00\n",
      "Epoch 76/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7882 - gamma: 2.00\n",
      "Epoch 77/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7804 - gamma: 2.00\n",
      "Epoch 78/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7905 - gamma: 2.00\n",
      "Epoch 79/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7887 - gamma: 2.00\n",
      "Epoch 80/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7985 - gamma: 2.00\n",
      "Epoch 81/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7766 - gamma: 2.00\n",
      "Epoch 82/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7901 - gamma: 2.00\n",
      "Epoch 83/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7954 - gamma: 2.00\n",
      "Epoch 84/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7902 - gamma: 2.00\n",
      "Epoch 85/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.7952 - gamma: 2.00\n",
      "Epoch 86/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7914 - gamma: 2.00\n",
      "Epoch 87/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8001 - gamma: 2.00\n",
      "Epoch 88/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8017 - gamma: 2.00\n",
      "Epoch 89/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8047 - gamma: 2.00\n",
      "Epoch 90/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.7984 - gamma: 2.00\n",
      "Epoch 91/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8070 - gamma: 2.00\n",
      "Epoch 92/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8089 - gamma: 2.00\n",
      "Epoch 93/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8026 - gamma: 2.00\n",
      "Epoch 94/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8150 - gamma: 2.00\n",
      "Epoch 95/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8104 - gamma: 2.00\n",
      "Epoch 96/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8015 - gamma: 2.00\n",
      "Epoch 97/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8113 - gamma: 2.00\n",
      "Epoch 98/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8144 - gamma: 2.00\n",
      "Epoch 99/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8064 - gamma: 2.00\n",
      "Epoch 100/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8110 - gamma: 2.00\n",
      "Epoch 101/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8108 - gamma: 2.00\n",
      "Epoch 102/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8176 - gamma: 2.00\n",
      "Epoch 103/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8295 - gamma: 2.00\n",
      "Epoch 104/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8142 - gamma: 2.00\n",
      "Epoch 105/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8169 - gamma: 2.00\n",
      "Epoch 106/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8258 - gamma: 2.00\n",
      "Epoch 107/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8259 - gamma: 2.00\n",
      "Epoch 108/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8211 - gamma: 2.00\n",
      "Epoch 109/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8268 - gamma: 2.00\n",
      "Epoch 110/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8207 - gamma: 2.00\n",
      "Epoch 111/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8245 - gamma: 2.00\n",
      "Epoch 112/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8295 - gamma: 2.00\n",
      "Epoch 113/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8242 - gamma: 2.00\n",
      "Epoch 114/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8260 - gamma: 2.00\n",
      "Epoch 115/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8285 - gamma: 2.00\n",
      "Epoch 116/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8262 - gamma: 2.00\n",
      "Epoch 117/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8254 - gamma: 2.00\n",
      "Epoch 118/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8391 - gamma: 2.00\n",
      "Epoch 119/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8201 - gamma: 2.00\n",
      "Epoch 120/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8359 - gamma: 2.00\n",
      "Epoch 121/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8273 - gamma: 2.00\n",
      "Epoch 122/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8356 - gamma: 2.00\n",
      "Epoch 123/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8328 - gamma: 2.00\n",
      "Epoch 124/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8385 - gamma: 2.00\n",
      "Epoch 125/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8288 - gamma: 2.00\n",
      "Epoch 126/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8345 - gamma: 2.00\n",
      "Epoch 127/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8458 - gamma: 2.00\n",
      "Epoch 128/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8382 - gamma: 2.00\n",
      "Epoch 129/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8361 - gamma: 2.00\n",
      "Epoch 130/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8393 - gamma: 2.00\n",
      "Epoch 131/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8460 - gamma: 2.00\n",
      "Epoch 132/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8395 - gamma: 2.00\n",
      "Epoch 133/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8479 - gamma: 2.00\n",
      "Epoch 134/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8431 - gamma: 2.00\n",
      "Epoch 135/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8479 - gamma: 2.00\n",
      "Epoch 136/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8508 - gamma: 2.00\n",
      "Epoch 137/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8385 - gamma: 2.00\n",
      "Epoch 138/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8267 - gamma: 2.00\n",
      "Epoch 139/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8440 - gamma: 2.00\n",
      "Epoch 140/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8355 - gamma: 2.00\n",
      "Epoch 141/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8367 - gamma: 2.00\n",
      "Epoch 142/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8461 - gamma: 2.00\n",
      "Epoch 143/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8486 - gamma: 2.00\n",
      "Epoch 144/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8427 - gamma: 2.00\n",
      "Epoch 145/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8557 - gamma: 2.00\n",
      "Epoch 146/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8437 - gamma: 2.00\n",
      "Epoch 147/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8567 - gamma: 2.00\n",
      "Epoch 148/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8498 - gamma: 2.00\n",
      "Epoch 149/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8448 - gamma: 2.00\n",
      "Epoch 150/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8446 - gamma: 2.00\n",
      "Epoch 151/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8429 - gamma: 2.00\n",
      "Epoch 152/300 - loss: 0.0000 - acc: 0.9984 - tss: 0.8526 - gamma: 2.00\n",
      "Epoch 153/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8528 - gamma: 2.00\n",
      "Epoch 154/300 - loss: 0.0000 - acc: 0.9984 - tss: 0.8499 - gamma: 2.00\n",
      "Epoch 155/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8465 - gamma: 2.00\n",
      "Epoch 156/300 - loss: 0.0000 - acc: 0.9984 - tss: 0.8489 - gamma: 2.00\n",
      "Epoch 157/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8260 - gamma: 2.00\n",
      "Early stopping triggered at epoch 157. Restoring best model from epoch 147.\n",
      "Model saved to models/EVEREST-v1.4-M5-72h\n",
      "Metrics: {\n",
      "  \"accuracy\": 0.9995259936706213,\n",
      "  \"precision\": 0.7916666666666666,\n",
      "  \"recall\": 0.9134615384615384,\n",
      "  \"f1\": 0.8482142857142857,\n",
      "  \"roc_auc\": 0.9999147536582093,\n",
      "  \"brier\": 0.0010672040128827474,\n",
      "  \"log_loss\": 0.022777514120353413,\n",
      "  \"tss\": 0.91311249823395\n",
      "}\n",
      "âœ… Saved results to ablation_results/full_model.json\n",
      "\n",
      "==================================================\n",
      "Running no_evt_head\n",
      "==================================================\n",
      "\n",
      "==============================\n",
      "â–¶ Running variant: no_evt_head\n",
      "==============================\n",
      "Epoch 1/300 - loss: 0.0001 - acc: 0.9932 - tss: 0.0160 - gamma: 0.00\n",
      "Epoch 2/300 - loss: 0.0000 - acc: 0.9939 - tss: 0.1271 - gamma: 0.04\n",
      "Epoch 3/300 - loss: 0.0000 - acc: 0.9942 - tss: 0.1779 - gamma: 0.08\n",
      "Epoch 4/300 - loss: 0.0000 - acc: 0.9944 - tss: 0.2222 - gamma: 0.12\n",
      "Epoch 5/300 - loss: 0.0000 - acc: 0.9947 - tss: 0.2732 - gamma: 0.16\n",
      "Epoch 6/300 - loss: 0.0000 - acc: 0.9949 - tss: 0.3308 - gamma: 0.20\n",
      "Epoch 7/300 - loss: 0.0000 - acc: 0.9952 - tss: 0.3675 - gamma: 0.24\n",
      "Epoch 8/300 - loss: 0.0000 - acc: 0.9955 - tss: 0.4043 - gamma: 0.28\n",
      "Epoch 9/300 - loss: 0.0000 - acc: 0.9956 - tss: 0.4159 - gamma: 0.32\n",
      "Epoch 10/300 - loss: 0.0000 - acc: 0.9956 - tss: 0.4232 - gamma: 0.36\n",
      "Epoch 11/300 - loss: 0.0000 - acc: 0.9959 - tss: 0.4685 - gamma: 0.40\n",
      "Epoch 12/300 - loss: 0.0000 - acc: 0.9959 - tss: 0.4766 - gamma: 0.44\n",
      "Epoch 13/300 - loss: 0.0000 - acc: 0.9960 - tss: 0.4924 - gamma: 0.48\n",
      "Epoch 14/300 - loss: 0.0000 - acc: 0.9961 - tss: 0.5233 - gamma: 0.52\n",
      "Epoch 15/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5336 - gamma: 0.56\n",
      "Epoch 16/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5427 - gamma: 0.60\n",
      "Epoch 17/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5481 - gamma: 0.64\n",
      "Epoch 18/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5593 - gamma: 0.68\n",
      "Epoch 19/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5640 - gamma: 0.72\n",
      "Epoch 20/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5648 - gamma: 0.76\n",
      "Epoch 21/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5861 - gamma: 0.80\n",
      "Epoch 22/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5953 - gamma: 0.84\n",
      "Epoch 23/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6062 - gamma: 0.88\n",
      "Epoch 24/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.6093 - gamma: 0.92\n",
      "Epoch 25/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6194 - gamma: 0.96\n",
      "Epoch 26/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6170 - gamma: 1.00\n",
      "Epoch 27/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6283 - gamma: 1.04\n",
      "Epoch 28/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6420 - gamma: 1.08\n",
      "Epoch 29/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6183 - gamma: 1.12\n",
      "Epoch 30/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6298 - gamma: 1.16\n",
      "Epoch 31/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6437 - gamma: 1.20\n",
      "Epoch 32/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6395 - gamma: 1.24\n",
      "Epoch 33/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6447 - gamma: 1.28\n",
      "Epoch 34/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6494 - gamma: 1.32\n",
      "Epoch 35/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6578 - gamma: 1.36\n",
      "Epoch 36/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6504 - gamma: 1.40\n",
      "Epoch 37/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6616 - gamma: 1.44\n",
      "Epoch 38/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6729 - gamma: 1.48\n",
      "Epoch 39/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6641 - gamma: 1.52\n",
      "Epoch 40/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6897 - gamma: 1.56\n",
      "Epoch 41/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6977 - gamma: 1.60\n",
      "Epoch 42/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6933 - gamma: 1.64\n",
      "Epoch 43/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6748 - gamma: 1.68\n",
      "Epoch 44/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6784 - gamma: 1.72\n",
      "Epoch 45/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6784 - gamma: 1.76\n",
      "Epoch 46/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.6993 - gamma: 1.80\n",
      "Epoch 47/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7080 - gamma: 1.84\n",
      "Epoch 48/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7098 - gamma: 1.88\n",
      "Epoch 49/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7060 - gamma: 1.92\n",
      "Epoch 50/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7272 - gamma: 1.96\n",
      "Epoch 51/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7234 - gamma: 2.00\n",
      "Epoch 52/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7173 - gamma: 2.00\n",
      "Epoch 53/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7347 - gamma: 2.00\n",
      "Epoch 54/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7312 - gamma: 2.00\n",
      "Epoch 55/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7403 - gamma: 2.00\n",
      "Epoch 56/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7524 - gamma: 2.00\n",
      "Epoch 57/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7442 - gamma: 2.00\n",
      "Epoch 58/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7448 - gamma: 2.00\n",
      "Epoch 59/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7339 - gamma: 2.00\n",
      "Epoch 60/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7461 - gamma: 2.00\n",
      "Epoch 61/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7606 - gamma: 2.00\n",
      "Epoch 62/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7601 - gamma: 2.00\n",
      "Epoch 63/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7581 - gamma: 2.00\n",
      "Epoch 64/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7684 - gamma: 2.00\n",
      "Epoch 65/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7665 - gamma: 2.00\n",
      "Epoch 66/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7810 - gamma: 2.00\n",
      "Epoch 67/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7765 - gamma: 2.00\n",
      "Epoch 68/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7760 - gamma: 2.00\n",
      "Epoch 69/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7762 - gamma: 2.00\n",
      "Epoch 70/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7760 - gamma: 2.00\n",
      "Epoch 71/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7615 - gamma: 2.00\n",
      "Epoch 72/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7790 - gamma: 2.00\n",
      "Epoch 73/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7876 - gamma: 2.00\n",
      "Epoch 74/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7741 - gamma: 2.00\n",
      "Epoch 75/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7762 - gamma: 2.00\n",
      "Epoch 76/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7985 - gamma: 2.00\n",
      "Epoch 77/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7920 - gamma: 2.00\n",
      "Epoch 78/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7943 - gamma: 2.00\n",
      "Epoch 79/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7973 - gamma: 2.00\n",
      "Epoch 80/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.7950 - gamma: 2.00\n",
      "Epoch 81/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8072 - gamma: 2.00\n",
      "Epoch 82/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7792 - gamma: 2.00\n",
      "Epoch 83/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8168 - gamma: 2.00\n",
      "Epoch 84/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.8055 - gamma: 2.00\n",
      "Epoch 85/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8015 - gamma: 2.00\n",
      "Epoch 86/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.8035 - gamma: 2.00\n",
      "Epoch 87/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8137 - gamma: 2.00\n",
      "Epoch 88/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8150 - gamma: 2.00\n",
      "Epoch 89/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8088 - gamma: 2.00\n",
      "Epoch 90/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8235 - gamma: 2.00\n",
      "Epoch 91/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8207 - gamma: 2.00\n",
      "Epoch 92/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8188 - gamma: 2.00\n",
      "Epoch 93/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8162 - gamma: 2.00\n",
      "Epoch 94/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8131 - gamma: 2.00\n",
      "Epoch 95/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8255 - gamma: 2.00\n",
      "Epoch 96/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8196 - gamma: 2.00\n",
      "Epoch 97/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8169 - gamma: 2.00\n",
      "Epoch 98/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8173 - gamma: 2.00\n",
      "Epoch 99/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8198 - gamma: 2.00\n",
      "Epoch 100/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8230 - gamma: 2.00\n",
      "Epoch 101/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8284 - gamma: 2.00\n",
      "Epoch 102/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8306 - gamma: 2.00\n",
      "Epoch 103/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8136 - gamma: 2.00\n",
      "Epoch 104/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8232 - gamma: 2.00\n",
      "Epoch 105/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8247 - gamma: 2.00\n",
      "Epoch 106/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8243 - gamma: 2.00\n",
      "Epoch 107/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8369 - gamma: 2.00\n",
      "Epoch 108/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8264 - gamma: 2.00\n",
      "Epoch 109/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8382 - gamma: 2.00\n",
      "Epoch 110/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8285 - gamma: 2.00\n",
      "Epoch 111/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8333 - gamma: 2.00\n",
      "Epoch 112/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8279 - gamma: 2.00\n",
      "Epoch 113/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8340 - gamma: 2.00\n",
      "Epoch 114/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8401 - gamma: 2.00\n",
      "Epoch 115/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8336 - gamma: 2.00\n",
      "Epoch 116/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8395 - gamma: 2.00\n",
      "Epoch 117/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8426 - gamma: 2.00\n",
      "Epoch 118/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8428 - gamma: 2.00\n",
      "Epoch 119/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8463 - gamma: 2.00\n",
      "Epoch 120/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8538 - gamma: 2.00\n",
      "Epoch 121/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8509 - gamma: 2.00\n",
      "Epoch 122/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8443 - gamma: 2.00\n",
      "Epoch 123/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8576 - gamma: 2.00\n",
      "Epoch 124/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8464 - gamma: 2.00\n",
      "Epoch 125/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8426 - gamma: 2.00\n",
      "Epoch 126/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8479 - gamma: 2.00\n",
      "Epoch 127/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8455 - gamma: 2.00\n",
      "Epoch 128/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8462 - gamma: 2.00\n",
      "Epoch 129/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8565 - gamma: 2.00\n",
      "Epoch 130/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8551 - gamma: 2.00\n",
      "Epoch 131/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8462 - gamma: 2.00\n",
      "Epoch 132/300 - loss: 0.0000 - acc: 0.9984 - tss: 0.8572 - gamma: 2.00\n",
      "Epoch 133/300 - loss: 0.0000 - acc: 0.9983 - tss: 0.8521 - gamma: 2.00\n",
      "Early stopping triggered at epoch 133. Restoring best model from epoch 123.\n",
      "Model saved to models/EVEREST-v1.5-M5-72h\n",
      "Metrics: {\n",
      "  \"accuracy\": 0.9987173946381519,\n",
      "  \"precision\": 0.5319148936170213,\n",
      "  \"recall\": 0.9615384615384616,\n",
      "  \"f1\": 0.684931506849315,\n",
      "  \"roc_auc\": 0.9991088736743187,\n",
      "  \"brier\": 0.0011358115689891095,\n",
      "  \"log_loss\": 0.010037519526680378,\n",
      "  \"tss\": 0.9603098401544181\n",
      "}\n",
      "âœ… Saved results to ablation_results/no_evt_head.json\n",
      "\n",
      "==================================================\n",
      "Running no_evidential_head\n",
      "==================================================\n",
      "\n",
      "==============================\n",
      "â–¶ Running variant: no_evidential_head\n",
      "==============================\n",
      "Epoch 1/300 - loss: 0.0001 - acc: 0.9932 - tss: 0.0145 - gamma: 0.00\n",
      "Epoch 2/300 - loss: 0.0000 - acc: 0.9938 - tss: 0.1174 - gamma: 0.04\n",
      "Epoch 3/300 - loss: 0.0000 - acc: 0.9941 - tss: 0.1645 - gamma: 0.08\n",
      "Epoch 4/300 - loss: 0.0000 - acc: 0.9944 - tss: 0.2203 - gamma: 0.12\n",
      "Epoch 5/300 - loss: 0.0000 - acc: 0.9947 - tss: 0.2856 - gamma: 0.16\n",
      "Epoch 6/300 - loss: 0.0000 - acc: 0.9950 - tss: 0.3356 - gamma: 0.20\n",
      "Epoch 7/300 - loss: 0.0000 - acc: 0.9952 - tss: 0.3682 - gamma: 0.24\n",
      "Epoch 8/300 - loss: 0.0000 - acc: 0.9953 - tss: 0.3931 - gamma: 0.28\n",
      "Epoch 9/300 - loss: 0.0000 - acc: 0.9955 - tss: 0.4190 - gamma: 0.32\n",
      "Epoch 10/300 - loss: 0.0000 - acc: 0.9957 - tss: 0.4533 - gamma: 0.36\n",
      "Epoch 11/300 - loss: 0.0000 - acc: 0.9958 - tss: 0.4680 - gamma: 0.40\n",
      "Epoch 12/300 - loss: 0.0000 - acc: 0.9959 - tss: 0.4825 - gamma: 0.44\n",
      "Epoch 13/300 - loss: 0.0000 - acc: 0.9960 - tss: 0.5103 - gamma: 0.48\n",
      "Epoch 14/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5274 - gamma: 0.52\n",
      "Epoch 15/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5437 - gamma: 0.56\n",
      "Epoch 16/300 - loss: 0.0000 - acc: 0.9963 - tss: 0.5455 - gamma: 0.60\n",
      "Epoch 17/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5767 - gamma: 0.64\n",
      "Epoch 18/300 - loss: 0.0000 - acc: 0.9965 - tss: 0.5751 - gamma: 0.68\n",
      "Epoch 19/300 - loss: 0.0000 - acc: 0.9965 - tss: 0.5791 - gamma: 0.72\n",
      "Epoch 20/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5829 - gamma: 0.76\n",
      "Epoch 21/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6064 - gamma: 0.80\n",
      "Epoch 22/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6063 - gamma: 0.84\n",
      "Epoch 23/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.6089 - gamma: 0.88\n",
      "Epoch 24/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6336 - gamma: 0.92\n",
      "Epoch 25/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6369 - gamma: 0.96\n",
      "Epoch 26/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6401 - gamma: 1.00\n",
      "Epoch 27/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6381 - gamma: 1.04\n",
      "Epoch 28/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6483 - gamma: 1.08\n",
      "Epoch 29/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6286 - gamma: 1.12\n",
      "Epoch 30/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6628 - gamma: 1.16\n",
      "Epoch 31/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6626 - gamma: 1.20\n",
      "Epoch 32/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6657 - gamma: 1.24\n",
      "Epoch 33/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6685 - gamma: 1.28\n",
      "Epoch 34/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6608 - gamma: 1.32\n",
      "Epoch 35/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6784 - gamma: 1.36\n",
      "Epoch 36/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6864 - gamma: 1.40\n",
      "Epoch 37/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6855 - gamma: 1.44\n",
      "Epoch 38/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6870 - gamma: 1.48\n",
      "Epoch 39/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6832 - gamma: 1.52\n",
      "Epoch 40/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6919 - gamma: 1.56\n",
      "Epoch 41/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7155 - gamma: 1.60\n",
      "Epoch 42/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7152 - gamma: 1.64\n",
      "Epoch 43/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7098 - gamma: 1.68\n",
      "Epoch 44/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7318 - gamma: 1.72\n",
      "Epoch 45/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7221 - gamma: 1.76\n",
      "Epoch 46/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7240 - gamma: 1.80\n",
      "Epoch 47/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7190 - gamma: 1.84\n",
      "Epoch 48/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7325 - gamma: 1.88\n",
      "Epoch 49/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7344 - gamma: 1.92\n",
      "Epoch 50/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7184 - gamma: 1.96\n",
      "Epoch 51/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7205 - gamma: 2.00\n",
      "Epoch 52/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7396 - gamma: 2.00\n",
      "Epoch 53/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7460 - gamma: 2.00\n",
      "Epoch 54/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7564 - gamma: 2.00\n",
      "Epoch 55/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7520 - gamma: 2.00\n",
      "Epoch 56/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7587 - gamma: 2.00\n",
      "Epoch 57/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7697 - gamma: 2.00\n",
      "Epoch 58/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7482 - gamma: 2.00\n",
      "Epoch 59/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7733 - gamma: 2.00\n",
      "Epoch 60/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7443 - gamma: 2.00\n",
      "Epoch 61/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7628 - gamma: 2.00\n",
      "Epoch 62/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7676 - gamma: 2.00\n",
      "Epoch 63/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7605 - gamma: 2.00\n",
      "Epoch 64/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7448 - gamma: 2.00\n",
      "Epoch 65/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7720 - gamma: 2.00\n",
      "Epoch 66/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7893 - gamma: 2.00\n",
      "Epoch 67/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7841 - gamma: 2.00\n",
      "Epoch 68/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7775 - gamma: 2.00\n",
      "Epoch 69/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7863 - gamma: 2.00\n",
      "Epoch 70/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7859 - gamma: 2.00\n",
      "Epoch 71/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7702 - gamma: 2.00\n",
      "Epoch 72/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7864 - gamma: 2.00\n",
      "Epoch 73/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7891 - gamma: 2.00\n",
      "Epoch 74/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7870 - gamma: 2.00\n",
      "Epoch 75/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7893 - gamma: 2.00\n",
      "Epoch 76/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7814 - gamma: 2.00\n",
      "Epoch 77/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7774 - gamma: 2.00\n",
      "Epoch 78/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7935 - gamma: 2.00\n",
      "Epoch 79/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7801 - gamma: 2.00\n",
      "Epoch 80/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8057 - gamma: 2.00\n",
      "Epoch 81/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8118 - gamma: 2.00\n",
      "Epoch 82/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7979 - gamma: 2.00\n",
      "Epoch 83/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8072 - gamma: 2.00\n",
      "Epoch 84/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8066 - gamma: 2.00\n",
      "Epoch 85/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8000 - gamma: 2.00\n",
      "Epoch 86/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.7984 - gamma: 2.00\n",
      "Epoch 87/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8015 - gamma: 2.00\n",
      "Epoch 88/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8031 - gamma: 2.00\n",
      "Epoch 89/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8110 - gamma: 2.00\n",
      "Epoch 90/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8098 - gamma: 2.00\n",
      "Epoch 91/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8093 - gamma: 2.00\n",
      "Early stopping triggered at epoch 91. Restoring best model from epoch 81.\n",
      "Model saved to models/EVEREST-v1.6-M5-72h\n",
      "Metrics: {\n",
      "  \"accuracy\": 0.9993168732311896,\n",
      "  \"precision\": 0.7433628318584071,\n",
      "  \"recall\": 0.8076923076923077,\n",
      "  \"f1\": 0.7741935483870968,\n",
      "  \"roc_auc\": 0.9998162169418714,\n",
      "  \"brier\": 0.0010217723026861069,\n",
      "  \"log_loss\": 0.00902064895300572,\n",
      "  \"tss\": 0.8072874210525509\n",
      "}\n",
      "âœ… Saved results to ablation_results/no_evidential_head.json\n",
      "\n",
      "==================================================\n",
      "Running no_precursor_head\n",
      "==================================================\n",
      "\n",
      "==============================\n",
      "â–¶ Running variant: no_precursor_head\n",
      "==============================\n",
      "Epoch 1/300 - loss: 0.0001 - acc: 0.9933 - tss: 0.0217 - gamma: 0.00\n",
      "Epoch 2/300 - loss: 0.0000 - acc: 0.9938 - tss: 0.1233 - gamma: 0.04\n",
      "Epoch 3/300 - loss: 0.0000 - acc: 0.9942 - tss: 0.1811 - gamma: 0.08\n",
      "Epoch 4/300 - loss: 0.0000 - acc: 0.9945 - tss: 0.2448 - gamma: 0.12\n",
      "Epoch 5/300 - loss: 0.0000 - acc: 0.9947 - tss: 0.2820 - gamma: 0.16\n",
      "Epoch 6/300 - loss: 0.0000 - acc: 0.9950 - tss: 0.3440 - gamma: 0.20\n",
      "Epoch 7/300 - loss: 0.0000 - acc: 0.9952 - tss: 0.3747 - gamma: 0.24\n",
      "Epoch 8/300 - loss: 0.0000 - acc: 0.9953 - tss: 0.3872 - gamma: 0.28\n",
      "Epoch 9/300 - loss: 0.0000 - acc: 0.9955 - tss: 0.3959 - gamma: 0.32\n",
      "Epoch 10/300 - loss: 0.0000 - acc: 0.9956 - tss: 0.4274 - gamma: 0.36\n",
      "Epoch 11/300 - loss: 0.0000 - acc: 0.9957 - tss: 0.4442 - gamma: 0.40\n",
      "Epoch 12/300 - loss: 0.0000 - acc: 0.9958 - tss: 0.4682 - gamma: 0.44\n",
      "Epoch 13/300 - loss: 0.0000 - acc: 0.9959 - tss: 0.4978 - gamma: 0.48\n",
      "Epoch 14/300 - loss: 0.0000 - acc: 0.9960 - tss: 0.5184 - gamma: 0.52\n",
      "Epoch 15/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5361 - gamma: 0.56\n",
      "Epoch 16/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5464 - gamma: 0.60\n",
      "Epoch 17/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5461 - gamma: 0.64\n",
      "Epoch 18/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5669 - gamma: 0.68\n",
      "Epoch 19/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5792 - gamma: 0.72\n",
      "Epoch 20/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5940 - gamma: 0.76\n",
      "Epoch 21/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6048 - gamma: 0.80\n",
      "Epoch 22/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5877 - gamma: 0.84\n",
      "Epoch 23/300 - loss: 0.0000 - acc: 0.9966 - tss: 0.5968 - gamma: 0.88\n",
      "Epoch 24/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6044 - gamma: 0.92\n",
      "Epoch 25/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6124 - gamma: 0.96\n",
      "Epoch 26/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6249 - gamma: 1.00\n",
      "Epoch 27/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6199 - gamma: 1.04\n",
      "Epoch 28/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6334 - gamma: 1.08\n",
      "Epoch 29/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6376 - gamma: 1.12\n",
      "Epoch 30/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6514 - gamma: 1.16\n",
      "Epoch 31/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6445 - gamma: 1.20\n",
      "Epoch 32/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6353 - gamma: 1.24\n",
      "Epoch 33/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6597 - gamma: 1.28\n",
      "Epoch 34/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6629 - gamma: 1.32\n",
      "Epoch 35/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6635 - gamma: 1.36\n",
      "Epoch 36/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6788 - gamma: 1.40\n",
      "Epoch 37/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6813 - gamma: 1.44\n",
      "Epoch 38/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6723 - gamma: 1.48\n",
      "Epoch 39/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6725 - gamma: 1.52\n",
      "Epoch 40/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6894 - gamma: 1.56\n",
      "Epoch 41/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6962 - gamma: 1.60\n",
      "Epoch 42/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.7003 - gamma: 1.64\n",
      "Epoch 43/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7156 - gamma: 1.68\n",
      "Epoch 44/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.7094 - gamma: 1.72\n",
      "Epoch 45/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.7032 - gamma: 1.76\n",
      "Epoch 46/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7145 - gamma: 1.80\n",
      "Epoch 47/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.7015 - gamma: 1.84\n",
      "Epoch 48/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7121 - gamma: 1.88\n",
      "Epoch 49/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7301 - gamma: 1.92\n",
      "Epoch 50/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7369 - gamma: 1.96\n",
      "Epoch 51/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7384 - gamma: 2.00\n",
      "Epoch 52/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7322 - gamma: 2.00\n",
      "Epoch 53/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7417 - gamma: 2.00\n",
      "Epoch 54/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7440 - gamma: 2.00\n",
      "Epoch 55/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7453 - gamma: 2.00\n",
      "Epoch 56/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7396 - gamma: 2.00\n",
      "Epoch 57/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7520 - gamma: 2.00\n",
      "Epoch 58/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7476 - gamma: 2.00\n",
      "Epoch 59/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7472 - gamma: 2.00\n",
      "Epoch 60/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7503 - gamma: 2.00\n",
      "Epoch 61/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7514 - gamma: 2.00\n",
      "Epoch 62/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7543 - gamma: 2.00\n",
      "Epoch 63/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7609 - gamma: 2.00\n",
      "Epoch 64/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7571 - gamma: 2.00\n",
      "Epoch 65/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7688 - gamma: 2.00\n",
      "Epoch 66/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7645 - gamma: 2.00\n",
      "Epoch 67/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7651 - gamma: 2.00\n",
      "Epoch 68/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7775 - gamma: 2.00\n",
      "Epoch 69/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7793 - gamma: 2.00\n",
      "Epoch 70/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7752 - gamma: 2.00\n",
      "Epoch 71/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7754 - gamma: 2.00\n",
      "Epoch 72/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7689 - gamma: 2.00\n",
      "Epoch 73/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7750 - gamma: 2.00\n",
      "Epoch 74/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7794 - gamma: 2.00\n",
      "Epoch 75/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7747 - gamma: 2.00\n",
      "Epoch 76/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7947 - gamma: 2.00\n",
      "Epoch 77/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7826 - gamma: 2.00\n",
      "Epoch 78/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7801 - gamma: 2.00\n",
      "Epoch 79/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7893 - gamma: 2.00\n",
      "Epoch 80/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7950 - gamma: 2.00\n",
      "Epoch 81/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8019 - gamma: 2.00\n",
      "Epoch 82/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7836 - gamma: 2.00\n",
      "Epoch 83/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7887 - gamma: 2.00\n",
      "Epoch 84/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8009 - gamma: 2.00\n",
      "Epoch 85/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.7975 - gamma: 2.00\n",
      "Epoch 86/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8005 - gamma: 2.00\n",
      "Epoch 87/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8015 - gamma: 2.00\n",
      "Epoch 88/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8012 - gamma: 2.00\n",
      "Epoch 89/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7954 - gamma: 2.00\n",
      "Epoch 90/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8024 - gamma: 2.00\n",
      "Epoch 91/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7898 - gamma: 2.00\n",
      "Epoch 92/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8043 - gamma: 2.00\n",
      "Epoch 93/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8011 - gamma: 2.00\n",
      "Epoch 94/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8148 - gamma: 2.00\n",
      "Epoch 95/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8083 - gamma: 2.00\n",
      "Epoch 96/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8003 - gamma: 2.00\n",
      "Epoch 97/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8102 - gamma: 2.00\n",
      "Epoch 98/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8199 - gamma: 2.00\n",
      "Epoch 99/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8205 - gamma: 2.00\n",
      "Epoch 100/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8070 - gamma: 2.00\n",
      "Epoch 101/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8245 - gamma: 2.00\n",
      "Epoch 102/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8184 - gamma: 2.00\n",
      "Epoch 103/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8316 - gamma: 2.00\n",
      "Epoch 104/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8264 - gamma: 2.00\n",
      "Epoch 105/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8260 - gamma: 2.00\n",
      "Epoch 106/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8123 - gamma: 2.00\n",
      "Epoch 107/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8269 - gamma: 2.00\n",
      "Epoch 108/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8237 - gamma: 2.00\n",
      "Epoch 109/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8220 - gamma: 2.00\n",
      "Epoch 110/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8203 - gamma: 2.00\n",
      "Epoch 111/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8144 - gamma: 2.00\n",
      "Epoch 112/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8300 - gamma: 2.00\n",
      "Epoch 113/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8304 - gamma: 2.00\n",
      "Early stopping triggered at epoch 113. Restoring best model from epoch 103.\n",
      "Model saved to models/EVEREST-v1.7-M5-72h\n",
      "Metrics: {\n",
      "  \"accuracy\": 0.9994144627695911,\n",
      "  \"precision\": 0.7384615384615385,\n",
      "  \"recall\": 0.9230769230769231,\n",
      "  \"f1\": 0.8205128205128205,\n",
      "  \"roc_auc\": 0.9993842126459928,\n",
      "  \"brier\": 0.000718371730954084,\n",
      "  \"log_loss\": 0.00655714987145587,\n",
      "  \"tss\": 0.9226022283981483\n",
      "}\n",
      "âœ… Saved results to ablation_results/no_precursor_head.json\n",
      "\n",
      "==================================================\n",
      "Running mean_pooling\n",
      "==================================================\n",
      "\n",
      "==============================\n",
      "â–¶ Running variant: mean_pooling\n",
      "==============================\n",
      "Epoch 1/300 - loss: 0.0001 - acc: 0.9927 - tss: 0.0245 - gamma: 0.00\n",
      "Epoch 2/300 - loss: 0.0000 - acc: 0.9938 - tss: 0.1218 - gamma: 0.04\n",
      "Epoch 3/300 - loss: 0.0000 - acc: 0.9942 - tss: 0.1843 - gamma: 0.08\n",
      "Epoch 4/300 - loss: 0.0000 - acc: 0.9945 - tss: 0.2370 - gamma: 0.12\n",
      "Epoch 5/300 - loss: 0.0000 - acc: 0.9947 - tss: 0.2889 - gamma: 0.16\n",
      "Epoch 6/300 - loss: 0.0000 - acc: 0.9951 - tss: 0.3478 - gamma: 0.20\n",
      "Epoch 7/300 - loss: 0.0000 - acc: 0.9952 - tss: 0.3647 - gamma: 0.24\n",
      "Epoch 8/300 - loss: 0.0000 - acc: 0.9954 - tss: 0.4019 - gamma: 0.28\n",
      "Epoch 9/300 - loss: 0.0000 - acc: 0.9955 - tss: 0.4257 - gamma: 0.32\n",
      "Epoch 10/300 - loss: 0.0000 - acc: 0.9956 - tss: 0.4427 - gamma: 0.36\n",
      "Epoch 11/300 - loss: 0.0000 - acc: 0.9957 - tss: 0.4541 - gamma: 0.40\n",
      "Epoch 12/300 - loss: 0.0000 - acc: 0.9960 - tss: 0.4955 - gamma: 0.44\n",
      "Epoch 13/300 - loss: 0.0000 - acc: 0.9960 - tss: 0.5088 - gamma: 0.48\n",
      "Epoch 14/300 - loss: 0.0000 - acc: 0.9962 - tss: 0.5307 - gamma: 0.52\n",
      "Epoch 15/300 - loss: 0.0000 - acc: 0.9963 - tss: 0.5342 - gamma: 0.56\n",
      "Epoch 16/300 - loss: 0.0000 - acc: 0.9963 - tss: 0.5543 - gamma: 0.60\n",
      "Epoch 17/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5610 - gamma: 0.64\n",
      "Epoch 18/300 - loss: 0.0000 - acc: 0.9964 - tss: 0.5567 - gamma: 0.68\n",
      "Epoch 19/300 - loss: 0.0000 - acc: 0.9965 - tss: 0.5756 - gamma: 0.72\n",
      "Epoch 20/300 - loss: 0.0000 - acc: 0.9965 - tss: 0.5810 - gamma: 0.76\n",
      "Epoch 21/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.5962 - gamma: 0.80\n",
      "Epoch 22/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.5929 - gamma: 0.84\n",
      "Epoch 23/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6086 - gamma: 0.88\n",
      "Epoch 24/300 - loss: 0.0000 - acc: 0.9967 - tss: 0.6006 - gamma: 0.92\n",
      "Epoch 25/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6175 - gamma: 0.96\n",
      "Epoch 26/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6187 - gamma: 1.00\n",
      "Epoch 27/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6292 - gamma: 1.04\n",
      "Epoch 28/300 - loss: 0.0000 - acc: 0.9968 - tss: 0.6246 - gamma: 1.08\n",
      "Epoch 29/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6450 - gamma: 1.12\n",
      "Epoch 30/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6443 - gamma: 1.16\n",
      "Epoch 31/300 - loss: 0.0000 - acc: 0.9969 - tss: 0.6611 - gamma: 1.20\n",
      "Epoch 32/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6635 - gamma: 1.24\n",
      "Epoch 33/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6618 - gamma: 1.28\n",
      "Epoch 34/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6647 - gamma: 1.32\n",
      "Epoch 35/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6612 - gamma: 1.36\n",
      "Epoch 36/300 - loss: 0.0000 - acc: 0.9970 - tss: 0.6668 - gamma: 1.40\n",
      "Epoch 37/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6803 - gamma: 1.44\n",
      "Epoch 38/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.6912 - gamma: 1.48\n",
      "Epoch 39/300 - loss: 0.0000 - acc: 0.9971 - tss: 0.6775 - gamma: 1.52\n",
      "Epoch 40/300 - loss: 0.0000 - acc: 0.9972 - tss: 0.7003 - gamma: 1.56\n",
      "Epoch 41/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7173 - gamma: 1.60\n",
      "Epoch 42/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7223 - gamma: 1.64\n",
      "Epoch 43/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7271 - gamma: 1.68\n",
      "Epoch 44/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7204 - gamma: 1.72\n",
      "Epoch 45/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7280 - gamma: 1.76\n",
      "Epoch 46/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7312 - gamma: 1.80\n",
      "Epoch 47/300 - loss: 0.0000 - acc: 0.9973 - tss: 0.7177 - gamma: 1.84\n",
      "Epoch 48/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7330 - gamma: 1.88\n",
      "Epoch 49/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7350 - gamma: 1.92\n",
      "Epoch 50/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7383 - gamma: 1.96\n",
      "Epoch 51/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7306 - gamma: 2.00\n",
      "Epoch 52/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7503 - gamma: 2.00\n",
      "Epoch 53/300 - loss: 0.0000 - acc: 0.9974 - tss: 0.7312 - gamma: 2.00\n",
      "Epoch 54/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7514 - gamma: 2.00\n",
      "Epoch 55/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7526 - gamma: 2.00\n",
      "Epoch 56/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7529 - gamma: 2.00\n",
      "Epoch 57/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7505 - gamma: 2.00\n",
      "Epoch 58/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7661 - gamma: 2.00\n",
      "Epoch 59/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7443 - gamma: 2.00\n",
      "Epoch 60/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7531 - gamma: 2.00\n",
      "Epoch 61/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7690 - gamma: 2.00\n",
      "Epoch 62/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7680 - gamma: 2.00\n",
      "Epoch 63/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7697 - gamma: 2.00\n",
      "Epoch 64/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7789 - gamma: 2.00\n",
      "Epoch 65/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7677 - gamma: 2.00\n",
      "Epoch 66/300 - loss: 0.0000 - acc: 0.9975 - tss: 0.7627 - gamma: 2.00\n",
      "Epoch 67/300 - loss: 0.0000 - acc: 0.9976 - tss: 0.7724 - gamma: 2.00\n",
      "Epoch 68/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7835 - gamma: 2.00\n",
      "Epoch 69/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7929 - gamma: 2.00\n",
      "Epoch 70/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7910 - gamma: 2.00\n",
      "Epoch 71/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7838 - gamma: 2.00\n",
      "Epoch 72/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7784 - gamma: 2.00\n",
      "Epoch 73/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7779 - gamma: 2.00\n",
      "Epoch 74/300 - loss: 0.0000 - acc: 0.9977 - tss: 0.7832 - gamma: 2.00\n",
      "Epoch 75/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8015 - gamma: 2.00\n",
      "Epoch 76/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7876 - gamma: 2.00\n",
      "Epoch 77/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7969 - gamma: 2.00\n",
      "Epoch 78/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7859 - gamma: 2.00\n",
      "Epoch 79/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8074 - gamma: 2.00\n",
      "Epoch 80/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7971 - gamma: 2.00\n",
      "Epoch 81/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.7990 - gamma: 2.00\n",
      "Epoch 82/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7912 - gamma: 2.00\n",
      "Epoch 83/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7958 - gamma: 2.00\n",
      "Epoch 84/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7826 - gamma: 2.00\n",
      "Epoch 85/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8055 - gamma: 2.00\n",
      "Epoch 86/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7962 - gamma: 2.00\n",
      "Epoch 87/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8055 - gamma: 2.00\n",
      "Epoch 88/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8080 - gamma: 2.00\n",
      "Epoch 89/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8019 - gamma: 2.00\n",
      "Epoch 90/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8034 - gamma: 2.00\n",
      "Epoch 91/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8091 - gamma: 2.00\n",
      "Epoch 92/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8017 - gamma: 2.00\n",
      "Epoch 93/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.8019 - gamma: 2.00\n",
      "Epoch 94/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8050 - gamma: 2.00\n",
      "Epoch 95/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8171 - gamma: 2.00\n",
      "Epoch 96/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8224 - gamma: 2.00\n",
      "Epoch 97/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8218 - gamma: 2.00\n",
      "Epoch 98/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8013 - gamma: 2.00\n",
      "Epoch 99/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8190 - gamma: 2.00\n",
      "Epoch 100/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8200 - gamma: 2.00\n",
      "Epoch 101/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8198 - gamma: 2.00\n",
      "Epoch 102/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8240 - gamma: 2.00\n",
      "Epoch 103/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8102 - gamma: 2.00\n",
      "Epoch 104/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8189 - gamma: 2.00\n",
      "Epoch 105/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8140 - gamma: 2.00\n",
      "Epoch 106/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8140 - gamma: 2.00\n",
      "Epoch 107/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8066 - gamma: 2.00\n",
      "Epoch 108/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8199 - gamma: 2.00\n",
      "Epoch 109/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8191 - gamma: 2.00\n",
      "Epoch 110/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8266 - gamma: 2.00\n",
      "Epoch 111/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8131 - gamma: 2.00\n",
      "Epoch 112/300 - loss: 0.0000 - acc: 0.9979 - tss: 0.8034 - gamma: 2.00\n",
      "Epoch 113/300 - loss: 0.0000 - acc: 0.9982 - tss: 0.8380 - gamma: 2.00\n",
      "Epoch 114/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8100 - gamma: 2.00\n",
      "Epoch 115/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8283 - gamma: 2.00\n",
      "Epoch 116/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8196 - gamma: 2.00\n",
      "Epoch 117/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8169 - gamma: 2.00\n",
      "Epoch 118/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8233 - gamma: 2.00\n",
      "Epoch 119/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8289 - gamma: 2.00\n",
      "Epoch 120/300 - loss: 0.0000 - acc: 0.9978 - tss: 0.7929 - gamma: 2.00\n",
      "Epoch 121/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8207 - gamma: 2.00\n",
      "Epoch 122/300 - loss: 0.0000 - acc: 0.9980 - tss: 0.8376 - gamma: 2.00\n",
      "Epoch 123/300 - loss: 0.0000 - acc: 0.9981 - tss: 0.8331 - gamma: 2.00\n",
      "Early stopping triggered at epoch 123. Restoring best model from epoch 113.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RETPlusModel' object has no attribute 'att_pool'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# Run this variant\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         metrics = \u001b[43mrun_ablation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_dir\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         all_results[name] = metrics\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Load all results (whether just generated or from previous runs)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mrun_ablation\u001b[39m\u001b[34m(variant_name, variant_kwargs, flare_class, time_window, input_shape, epochs, batch_size, results_dir)\u001b[39m\n\u001b[32m     71\u001b[39m X_test, y_test = get_testing_data(\u001b[38;5;28mstr\u001b[39m(time_window), flare_class)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma_max\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     85\u001b[39m y_prob = model.predict_proba(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/masters-project/models/solarknowledge_ret_plus.py:498\u001b[39m, in \u001b[36mRETPlusWrapper.train\u001b[39m\u001b[34m(self, X_train, y_train, epochs, batch_size, gamma_max, warmup_epochs, flare_class, time_window)\u001b[39m\n\u001b[32m    495\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Save best model & metadata\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m model_dir = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Provide evaluation data for artefact generation\u001b[39;49;00m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_dir\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/masters-project/models/solarknowledge_ret_plus.py:623\u001b[39m, in \u001b[36mRETPlusWrapper.save\u001b[39m\u001b[34m(self, version, flare_class, time_window, X_eval, y_eval)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m model_dir = \u001b[43msave_model_with_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_shape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membed_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_heads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mff_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_blocks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdropout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflare_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEVEREST model trained on SHARP data with evidential and EVT losses.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Newly added artefacts\u001b[39;49;00m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevt_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevt_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43matt_X_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43matt_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43matt_y_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43matt_y_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43matt_y_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[43matt_y_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43matt_y_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43matt_y_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevidential_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevid_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_dir\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/masters-project/models/model_tracking.py:210\u001b[39m, in \u001b[36msave_model_with_metadata\u001b[39m\u001b[34m(model, metrics, hyperparams, history, version, flare_class, time_window, description, y_true, y_pred, y_scores, evt_scores, sample_input, latency_repeats, att_X_batch, att_y_true, att_y_pred, att_y_score, evidential_out)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Attention heatmaps\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     (att_X_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (att_y_true \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (att_y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (att_y_score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    209\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[43msave_attention_heatmaps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43matt_X_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43matt_y_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43matt_y_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43matt_y_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Evidential uncertainty violin plot\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (evidential_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (y_true \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repositories/masters-project/models/model_tracking.py:687\u001b[39m, in \u001b[36msave_attention_heatmaps\u001b[39m\u001b[34m(model, X_batch, y_true, y_pred, y_score, model_dir, max_samples)\u001b[39m\n\u001b[32m    684\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m model.model.transformers:\n\u001b[32m    685\u001b[39m         x = blk(x)\n\u001b[32m    686\u001b[39m     att_weights = (\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43matt_pool\u001b[49m(x).squeeze(-\u001b[32m1\u001b[39m).cpu().numpy()\n\u001b[32m    688\u001b[39m     )  \u001b[38;5;66;03m# shape (B, T)\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_batch), max_samples)):\n\u001b[32m    691\u001b[39m     plt.figure(figsize=(\u001b[32m6\u001b[39m, \u001b[32m1.5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/everest_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'RETPlusModel' object has no attribute 'att_pool'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "from solarknowledge_ret_plus import RETPlusWrapper\n",
    "from utils import get_training_data, get_testing_data\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "def evaluate_predictions(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    y_prob = y_prob.flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    tss = (cm[1, 1] / (cm[1, 1] + cm[1, 0] + 1e-8)) + (\n",
    "        cm[0, 0] / (cm[0, 0] + cm[0, 1] + 1e-8)\n",
    "    ) - 1\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
    "        \"brier\": brier_score_loss(y_true, y_prob),\n",
    "        \"log_loss\": log_loss(y_true, y_prob.clip(1e-6, 1 - 1e-6)),\n",
    "        \"tss\": tss,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def run_ablation(\n",
    "    variant_name: str,\n",
    "    variant_kwargs: Dict,\n",
    "    flare_class: str = \"M5\",\n",
    "    time_window: str = \"72\",\n",
    "    input_shape=(10, 9),\n",
    "    epochs: int = 300,\n",
    "    batch_size: int = 512,\n",
    "    results_dir: str = \"ablation_results\",\n",
    "):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    result_path = os.path.join(results_dir, f\"{variant_name}.json\")\n",
    "    if os.path.exists(result_path):\n",
    "        print(f\"âœ… Skipping {variant_name} â€” already evaluated.\")\n",
    "        with open(result_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"\\n==============================\\nâ–¶ Running variant: {variant_name}\\n==============================\")\n",
    "\n",
    "    wrapper_kwargs = {k: v for k, v in variant_kwargs.items() if k not in [\"gamma_max\"]}\n",
    "    train_kwargs = {k: v for k, v in variant_kwargs.items() if k in [\"gamma_max\"]}\n",
    "\n",
    "    model = RETPlusWrapper(input_shape, **wrapper_kwargs)\n",
    "\n",
    "    X_train, y_train = get_training_data(str(time_window), flare_class)\n",
    "    X_test, y_test = get_testing_data(str(time_window), flare_class)\n",
    "\n",
    "    model.train(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        gamma_max=train_kwargs.get(\"gamma_max\", 2.0),\n",
    "        flare_class=flare_class,\n",
    "        time_window=int(time_window),\n",
    "    )\n",
    "\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    metrics = evaluate_predictions(np.array(y_test), y_prob)\n",
    "    print(\"Metrics:\", json.dumps(metrics, indent=2))\n",
    "\n",
    "    with open(result_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"âœ… Saved results to {result_path}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "flare_class = \"M5\"\n",
    "time_window = \"72\"\n",
    "input_shape = (10, 9)\n",
    "epochs = 300\n",
    "batch_size = 512\n",
    "results_dir = \"ablation_results\"\n",
    "\n",
    "variants = [\n",
    "    {\"name\": \"full_model\", \"kwargs\": {}},\n",
    "    {\"name\": \"no_evt_head\", \"kwargs\": {\"use_evt\": False, \"loss_weights\": {\"focal\": 0.9, \"evid\": 0.1, \"evt\": 0.0, \"prec\": 0.05}}},\n",
    "    {\"name\": \"no_evidential_head\", \"kwargs\": {\"use_evidential\": False, \"loss_weights\": {\"focal\": 0.9, \"evid\": 0.0, \"evt\": 0.1, \"prec\": 0.05}}},\n",
    "    {\"name\": \"no_precursor_head\", \"kwargs\": {\"use_precursor\": False}},\n",
    "    {\"name\": \"mean_pooling\", \"kwargs\": {\"use_attention_bottleneck\": False}},\n",
    "    {\"name\": \"focal_only_loss\", \"kwargs\": {\"loss_weights\": {\"focal\": 1.0, \"evid\": 0.0, \"evt\": 0.0, \"prec\": 0.0}}},\n",
    "    {\"name\": \"no_gamma_annealing\", \"kwargs\": {\"gamma_max\": 0.0}},\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "RUN_ABLATIONS = True\n",
    "\n",
    "if RUN_ABLATIONS:\n",
    "    for variant in variants:\n",
    "        name = variant[\"name\"]\n",
    "        kw = variant.get(\"kwargs\", {})\n",
    "        print(f\"\\n{'='*50}\\nRunning {name}\\n{'='*50}\")\n",
    "        metrics = run_ablation(\n",
    "            name,\n",
    "            kw,\n",
    "            flare_class=flare_class,\n",
    "            time_window=time_window,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            results_dir=results_dir\n",
    "        )\n",
    "        all_results[name] = metrics\n",
    "\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        variant_name = filename.split(\".\")[0]\n",
    "        if variant_name not in all_results:\n",
    "            with open(os.path.join(results_dir, filename), 'r') as f:\n",
    "                all_results[variant_name] = json.load(f)\n",
    "\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "\n",
    "# Display\n",
    "display(results_df.style.highlight_max(axis=0))\n",
    "\n",
    "# Plot key metrics\n",
    "metrics_to_plot = ['tss', 'roc_auc', 'f1', 'brier']\n",
    "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(12, 4*len(metrics_to_plot)))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    if metric in results_df.columns:\n",
    "        ax = axes[i] if len(metrics_to_plot) > 1 else axes\n",
    "        results_df[metric].plot(kind='bar', ax=ax, color='skyblue')\n",
    "        ax.set_title(f'{metric.upper()} by Model Variant')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax.set_ylim([results_df[metric].min() * 0.95, min(1.0, results_df[metric].max() * 1.05)])\n",
    "        for j, v in enumerate(results_df[metric]):\n",
    "            ax.text(j, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Radar chart\n",
    "metrics_for_radar = ['tss', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "df_radar = results_df[metrics_for_radar].copy()\n",
    "for col in df_radar.columns:\n",
    "    df_radar[col] = (df_radar[col] - df_radar[col].min()) / (df_radar[col].max() - df_radar[col].min())\n",
    "\n",
    "N = len(metrics_for_radar)\n",
    "angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "plt.xticks(angles[:-1], metrics_for_radar, size=12)\n",
    "\n",
    "for variant, values in df_radar.iterrows():\n",
    "    values_list = values.tolist()\n",
    "    values_list += values_list[:1]\n",
    "    ax.plot(angles, values_list, linewidth=2, label=variant)\n",
    "    ax.fill(angles, values_list, alpha=0.1)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "plt.title('Normalized Performance Metrics Across Model Variants', size=15)\n",
    "plt.show()\n",
    "\n",
    "results_df.to_csv(f'ablation_results_{flare_class}_{time_window}.csv')\n",
    "print(f\"Saved summary to ablation_results_{flare_class}_{time_window}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVEREST Environment",
   "language": "python",
   "name": "everest_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
